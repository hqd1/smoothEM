require(fda)
require(plotly)
require(RColorBrewer)
# This file contain R code for the functions needed in the smooth.spikes project
# The first function that we need is that we use to generate data
# This function return a list of 4 elements: 
## spikes.index: a vector of spikes index. Each element takes value between 1 and length(grid), where grid is created from 'range' and 'by' (and sometimes from spikes generated as well.)
## spikes.loc: the actual location of spikes. Each element takes value between range[1] and range[2]
## smooth: the smooth part evaluated at grid
## data: a data frame. y is the actual data we want to analyze. grid is, well, grid.
generate.data = function(range, by = 0.01,poly.coefs,nspikes=NA,spikes.loc.dist="homopp",spikes.loc.param=NA, arg.others = NA,spikes.dist="fixed",spikes.param=NA, noise.sd,plot){
  # Caution: if generating spikes according to homopp or nonhomopp, we generate the location of spikes, then move these locations to the nearest grid points
  ## If there are more than 1 spikes between 2 grid points, they are merged. Thus, the smaller the grid, the more accurate the number of spikes is
  # range: is the domain of the polynomials
  # by: the binning over the range, default to 0.01
  # poly.coefs is the coefficients of underlying polynomial curves, of order from low to high
  ## that is: poly.coefs[1] is the coefficient of degree zero term. 
  # spikes.loc.dist: distribution of the spikes location, default to "homopp"
  ## if spikes.dist == fixed, that is height of spikes is constant, spikes.loc.dist can be "uniform", "homopp", "nonhomopp"
  ## if spikes.dist is random, spikes.loc.dist must be either "homopp" or "nonhomopp"
  # spikes.loc.param: 
  ## in HomoPP, this is the rate of interarrival time; if not specified, must specify 'nspikes' and the code will give approximately 'nspikes' spikes.
  ## in NonHomoPP, this is the mean function lambda(x). note that the mean function must be of the same length as the grid generated by "range" and "by"
  ## in Normal: this is a vector c(mean,sd) of the normal distribution
  # arg.others:
  ## if spikes.loc.param is a function, arg.others are extra arguments to be fed into the function. If there are more than 1 extra arguments, must be a vector.
  # spikes.dist: distribution of the spikes height, default to 'fixed', ie constant
  ## if spikes.loc.dist == "homopp", spikes.dist can be "fixed", "normal", "chisq", "ncchisq", or a function that generate spikes height.
  ### If a function, the first argument must be the approximate number of spikes user wants to generate. Other arguments necessary must be contained in spikes.param in correct order.
  ## if spikes.loc.dist != "homopp", spikes.dist must be "fixed". We might change this later.
  # spikes.param: the parameters of the distribution for spikes height.
  ## if spikes.dist == "fixed", spikes.param is the constant height of spikes
  ## if spikes.dist is random, spikes.param is the (vector of) parameter(s) for 'spikes.dist'
  # noise.sd: the sd of white noise
  # plot: if 'yes', plot the generated data
  # Caution2: if spikes.dist == fixed, i.e. spikes height is a constant, the function returns constant height. If spikes.dist != fixed, the function returns a vector of height values for each observations
  if(spikes.dist == "fixed"){ #Case 1: fixed height for spikes
    if(length(spikes.param)>1) warning('\'spikes.param\' has multiple elements. Only the first element will be used.')
    height = spikes.param[1]
    if(spikes.loc.dist == "uniform"){
      # check range
      if (diff(range) <= 0) stop('Invalid \'range\'.')
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes location
      spikes.loc = runif(nspikes, min = range[1], max = range[2])    #spikes location/index in grid rather
      for (i in (1:length(spikes.loc))){ #This function merge the spike locations with their nearest grids. If there're more than 2 spikes
        # between 2 consecutive grid points, they are merged.
        spikes.loc[i] = grid[which.min(abs(grid-spikes.loc[i]))]
      }
      spikes.loc=unique(spikes.loc)
      spikes.index = which(grid %in% spikes.loc)  
      # Generate the polynomial part
      degree = length(poly.coefs) - 1
      smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      # Generate data
      ngrid = length(grid)
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      # Plot data if user opts to 
      p = NA
      if(plot == "yes"){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )
        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "markers", 
                    marker = list(color = jBrewColors[2], size = 5, symbol = 'o'))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 5))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 5))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
        # layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        # par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
        # plot(grid,smooth, type = "l", ylim = c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "")
        # title(sub = "Curves + Random Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
        # points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        # plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        # title(main = "Data: Curves + Random Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
        p
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc,smooth = smooth, data = data, height = height, plot = p)
      return(output)
    }
    if(spikes.loc.dist == "homopp"){
      # Spikes from Homogeneous Poisson Process
      if(any(is.na(spikes.loc.param))){
        if (is.na(nspikes)) stop('Must specify either \'spikes.loc.param\' or \'nspikes\'.')
        warning('Approximately ', nspikes, ' spikes will be generated.')
        want.nspikes = nspikes  #approximately how many spikes we want to generate
        pp.rate = want.nspikes/diff(range)  ##higher rate leads to higher nspikes
      }
      if(all(!is.na(spikes.loc.param))){
        if (spikes.loc.param[1] <= 0) stop('Invalid \'spikes.loc.param\'.')
        if (length(spikes.loc.param) > 1) warning('\'spikes.loc.param\' has multiple elements. Only the first element will be used')
        pp.rate = spikes.loc.param[1]
      }
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes
      ## let numerator be approximate the number of spikes we want
      ## denominator is the range of x axis
      want.nspikes = pp.rate*diff(range) #in case spikes.loc.param is specified
      wait = rexp(want.nspikes + 100, rate = pp.rate)  #wait times of homogeneous poisson process are independent exponential
      ## we are generating more than we need
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes
      spikes.loc= cumsum(wait)
      spikes.loc = spikes.loc - (min(spikes.loc) - range[1])  ##transform spikes location to have them in the range
      spikes.loc = spikes.loc[spikes.loc < range[2]]  ##remove those beyond the range
      for (i in (1:length(spikes.loc))){ #This function merge the spike locations with their nearest grids. If there're more than 2 spikes
        # between 2 consecutive grid points, they are merged.
        spikes.loc[i] = grid[which.min(abs(grid-spikes.loc[i]))]
      }
      spikes.loc=unique(spikes.loc)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial
      degree = length(poly.coefs) - 1
      smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      # Generate data
      ngrid = length(grid)
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      # if(plot == "yes"){
      #   layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
      #   par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
      #   plot(grid,smooth, type = "l", ylim = c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "")
      #   title(sub = "Curves + HMPP Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
      #   points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
      #   plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
      #   title(main = "Data: Curves + HMPP Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      # }
      p = NA
      if(plot == "yes"){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )
        
        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "markers", 
                    marker = list(color = jBrewColors[2], size = 5, symbol = 'o'))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 5))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 5))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height, plot = p)
      return(output)
    }
    if(spikes.loc.dist == "nonhomopp"){
      ## Using thinning method by Lewis and Shedler (1978)
      ## Doesn't require integration of the mean function, so it's good even for complicated mean functions that are hard to integrate
      ## However we do need to be able to evaluate the mean function at the grid
      ## Generate non-homogeneous Poisson process (NHPP)
      ## First, generate a homogeneous Poisson process with rate lambda_h = max(lambda), where h stands for homogeneous, the non-homogeneous is suppressed from lambda_nh to be just lambda
      grid = seq(range[1],range[2],by = by)
      if (!is.function(spikes.loc.param)) stop('spikes.loc.param must be a function.')
      if(is.na(arg.others)) lambda = sapply(grid,spikes.loc.param) else lambda=sapply(grid,spikes.loc.param,arg.others)
      lambda.h = max(lambda)
      wait.h = rexp(100000, rate = lambda.h)  ## wait times of homogeneous poisson process are independent exponential
      ## we are generating 10000 wait times, we can adjust this number later
      spikes.h.loc = cumsum(wait.h) ## spikes.h.loc are spikes from homogeneous Poisson, candidates for deletion (or NHPP, depending on how you view it)
      for (i in (1:length(spikes.h.loc))){
        spikes.h.loc[i] = grid[which.min(abs(grid-spikes.h.loc[i]))]
      }
      spikes.h.loc=unique(spikes.h.loc)
      spikes.h.index = which(grid %in% spikes.h.loc)
      ## thinning requires knowing the ratio of lambda/lambda.h
      ## If if turns out that lambda[spikes.h] > lambda.h, that's okay, it means that we will not delete spikes at these locations.
      nspikes.h = length(spikes.h.loc)
      spikes.loc = c()
      i = 1
      while(i<=nspikes.h){
        if (runif(1,0,1)< (lambda[spikes.h.index[i]]/lambda.h)) spikes.loc = c(spikes.loc,spikes.h.loc[i])
        i = i+1
      }
      spikes.loc = spikes.loc[spikes.loc >= range[1]]  #we don't transform the spikes.loc here, because if we do so it will mismatch with the mean function
      spikes.loc = spikes.loc[spikes.loc <= range[2]]  
      ## note that we are not doing (spikes.loc - range[1])/range[2], which will shrink the locations and make them not match with lambda later
      ## generate the final grid with NH-Poisson spikes locations
      ## I want to keep the naming of the variables used to generate final data the same as in other sections, so that the analysis part of the code can be recycled 
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial
      degree = length(poly.coefs) - 1
      smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      ngrid = length(grid)
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      p = NA
      # if(plot == "yes" & is.na(arg.others)){
      #   layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
      #   par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
      #   #plot(grid,smooth, type = "l", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "")
      #   plot(grid,smooth, type = "l", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "")
      #   title(sub = "Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
      #   points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
      #   #plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
      #   plot(data$grid, data$y, type = "p", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
      #   title(main = "Data: Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      #   lines(grid, spikes.loc.param(grid), col = "green") #this is lambda
      # }
      # if(plot == "yes" & !is.na(arg.others)){
      #   layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
      #   par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
      #   #plot(grid,smooth, type = "l", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "")
      #   plot(grid,smooth, type = "l", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "")
      #   title(sub = "Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
      #   points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
      #   #plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
      #   plot(data$grid, data$y, type = "p", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
      #   title(main = "Data: Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      #   lines(grid, spikes.loc.param(grid, arg.others), col = "green") #this is lambda
      # }
      if(plot == "yes" & is.na(arg.others)){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )
        
        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines", 
                    line = list(color = jBrewColors[2], size = 3))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid,y = ~spikes.loc.param(grid), name = 'mean function',
                    mode = 'lines',
                    line = list(color = jBrewColors[5], size = 3))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
      }
      if(plot == "yes" & !is.na(arg.others)){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )
        
        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines", 
                    line = list(color = jBrewColors[2], size = 3))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers', 
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid,y = ~sapply(grid,spikes.loc.param, arg.others), name = 'mean function',
                    mode = 'lines',
                    line = list(color = jBrewColors[5], size = 3))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
        
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height, plot = p)
      return(output)
    }
    if(spikes.loc.dist == "normal"){
      if(any(is.na(spikes.loc.param))) stop('Need \'spikes.loc.param\'.')
      if(all(!is.na(spikes.loc.param))){
        if (length(spikes.loc.param) != 2) stop('\'spikes.loc.param\' must be of length 2.')
        if (spikes.loc.param[2] < 0) stop('Invalid \'spikes.loc.param\', sd must be positive')
        if ((spikes.loc.param[1] - 3*spikes.loc.param[2]) < range[1]|(spikes.loc.param[1] + 2*spikes.loc.param[2]) > range[2]) warning('Large prior sd might result in excluding out-of-range spikes')
      }
      prior.mean = spikes.loc.param[1]
      prior.sd = spikes.loc.param[2]
      ###Generate the spikes
      grid = seq(range[1],range[2],by = by)
      spikes.loc = rnorm(nspikes, mean = prior.mean, sd = prior.sd)    #spikes location/index in grid rather
      spikes.loc = spikes.loc - (min(spikes.loc) - range[1])  ##transform spikes location to have them in the range
      spikes.loc = spikes.loc[spikes.loc < range[2]]  ##remove those beyond the range
      grid = sort(unique(c(grid,spikes.loc)), decreasing = FALSE)
      ngrid = length(grid)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial part
      degree = length(poly.coefs) - 1
      smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      ###generate the data
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      if(plot == "yes"){
        layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
        plot(grid,smooth, type = "l", ylim = c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "")
        title(sub = "Curves + Random Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = 2.5)
        points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        plot(data$grid, data$y, type = "p", ylim=c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        title(main = "Data: Curves + Random Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height)
      return(output)
    }
    stop('\'spikes.loc.dist\' other than \'uniform\',\'homopp\',\'nonhomopp\',\'normal\' is not available.')
  }
  # From here on is code for random spikes height, for now we only consider homopp and nonhomopp distribution for spikes location
  if(spikes.dist != "fixed"){
    if(spikes.loc.dist == "homopp"){
      height.gen=NA #If height.gen is unchanged in the next few 'if' cases, it means user enters an invalid spikes.dist
      if(spikes.dist == "normal"){
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) != 2) stop('\'spikes.param\' must be of length 2.')
          if (spikes.param[2] < 0) stop('Invalid \'spikes.param\', sd must be positive.')
        }
        height.mean = spikes.param[1]
        height.sd = spikes.param[2]
        height.gen = function(nspikes){
          return(rnorm(nspikes, height.mean, height.sd))
        }
      }
      if(spikes.dist == "chisq"){
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) > 1) warning('\'spikes.param\' has multiple elements. Only the first element will be used.')
          if (spikes.param[1] < 0) stop('Invalid \'spikes.param\', df must be positive.')
        }
        height.df = spikes.param[1]
        height.gen = function(nspikes){
          return(rchisq(nspikes,height.df))
        }
      }
      if(spikes.dist == "ncchisq"){ #i.e., non-central chi squared
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) != 2) stop('\'spikes.param\' need to be of length 2.')
          if (any(spikes.param < 0)) stop('Invalid \'spikes.param\', df and ncp must be positive.')
        }
        height.df = spikes.param[1]
        height.ncp = spikes.param[2]
        height.gen = function(nspikes){
          return(rchisq(nspikes,height.df,height.ncp))
        }
      }
      if(is.function(spikes.dist)){ #user specified heights
        warning('In \'spikes.dist\': First argument must be number of spikes to generate, others arguments contained in \'spikes.param\'.')
        height.gen = spikes.dist(nspikes,spikes.param)
      }
      if(!is.function(height.gen)) stop('Invalid \'spikes.dist\'.')
      if(any(is.na(spikes.loc.param))){
        if (is.na(nspikes)) stop('Must specify either \'spikes.loc.param\' or \'nspikes\'.')
        warning('Approximately ', nspikes, ' spikes will be generated.')
        want.nspikes = nspikes  #approximately how many spikes we want to generate
        pp.rate = want.nspikes/diff(range)  ##higher rate leads to higher nspikes
      }
      if(all(!is.na(spikes.loc.param))){
        if (spikes.loc.param[1] <= 0) stop('Invalid \'spikes.loc.param\'.')
        if (length(spikes.loc.param) > 1) warning('\'spikes.loc.param\' has multiple elements. Only the first element will be used')
        pp.rate = spikes.loc.param[1]
      }
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes
      ## let numerator be approximate the number of spikes we want
      ## denominator is the range of x axis
      want.nspikes = pp.rate*diff(range) #in case spikes.loc.param is specified
      wait = rexp(want.nspikes + 100, rate = pp.rate)  #wait times of homogeneous poisson process are independent exponential
      ## we are generating more than we need
      spikes.loc= cumsum(wait)
      spikes.loc = spikes.loc - (min(spikes.loc) - range[1])  ##transform spikes location to have them in the range
      spikes.loc = spikes.loc[spikes.loc < range[2]]  ##remove those beyond the range
      for (i in (1:length(spikes.loc))){
        spikes.loc[i] = grid[which.min(abs(grid-spikes.loc[i]))]
      }
      spikes.loc=unique(spikes.loc)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial
      degree = length(poly.coefs) - 1
      smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      # Generate data
      ngrid = length(grid)
      nspikes = length(spikes.index)
      y = smooth+rnorm(ngrid,0,noise.sd)
      height = height.gen(nspikes)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      if(plot == "yes"){
        layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
        plot(grid,smooth, type = "l", ylim = c(min(smooth) - max(abs(height)), max(smooth)+max(abs(height))), main = "", xlab = "", ylab = "")
        title(sub = "Curves + HMPP Spikes w/ Normal Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
        points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - max(abs(height)), max(smooth)+max(abs(height))), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        title(main = "Data: Curves + HMPP Spikes w/ Normal Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data)
      return(output)
    }
    if(spikes.loc.dist == "nonhomopp"){
      ## Using thinning method by Lewis and Shedler (1978)
      ## Doesn't require integration of the mean function, so it's good even for complicated mean functions that are hard to integrate
      ## However we do need to be able to evaluate the mean function at the grid
      ## Generate non-homogeneous Poisson process (NHPP)
      if(spikes.dist != "normal"){
        stop('for nonhomopp \'spikes.loc.dist\', \'spikes.dist\' can only be \'normal\' or \'fixed\'')
      }
      if(spikes.dist == "normal"){
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) != 2) stop('\'spikes.param\' must be of length 2.')
          if (spikes.param[2] < 0) stop('Invalid \'spikes.param\', sd must be positive.')
        }
        height.mean = spikes.param[1]
        height.sd = spikes.param[2]
        # height.gen = function(nspikes){
        #   return(rnorm(nspikes, height.mean, height.sd))
        ## First, generate a homogeneous Poisson process with rate lambda_h = max(lambda), where h stands for homogeneous, the non-homogeneous is suppressed from lambda_nh to be just lambda
        ## generate the first grid, lambda will be calculated using this grid
        grid = seq(range[1],range[2],by = by)
        if (!is.function(spikes.loc.param)) stop('spikes.loc.param must be a function.')
        if(is.na(arg.others)) lambda = sapply(grid,spikes.loc.param) else lambda=sapply(grid,spikes.loc.param,arg.others)
        #if(is.na(arg.others)) lambda = spikes.loc.param(grid) else lambda = spikes.loc.param(grid, arg.others)
        lambda.h = max(lambda)
        wait.h = rexp(100000, rate = lambda.h)  ## wait times of homogeneous poisson process are independent exponential
        ## we are generating 1000000 wait times, we can adjust this number later
        spikes.h.loc = cumsum(wait.h) ## spikes.h.loc are spikes from homogeneous Poisson, candidates for deletion (or NHPP, depending on how you view it)
        for (i in (1:length(spikes.h.loc))){
          spikes.h.loc[i] = grid[which.min(abs(grid-spikes.h.loc[i]))]
        }
        spikes.h.loc=unique(spikes.h.loc)
        spikes.h.index = which(grid %in% spikes.h.loc)
        ## thinning requires knowing the ratio of lambda/lambda.h
        ## If if turns out that lambda[spikes.h] > lambda.h, that's okay, it means that we will not delete spikes at these locations.
        nspikes.h = length(spikes.h.loc)
        spikes.loc = c()
        i = 1
        while(i<=nspikes.h){
          if (runif(1,0,1)< (lambda[spikes.h.index[i]]/lambda.h)) spikes.loc = c(spikes.loc,spikes.h.loc[i])
          i = i+1
        }
        spikes.loc = spikes.loc[spikes.loc >= range[1]]  #we don't transform the spikes.loc here, because if we do so it will mismatch with the mean function
        spikes.loc = spikes.loc[spikes.loc <= range[2]]  
        ## note that we are not doing (spikes.loc - range[1])/range[2], which will shrink the locations and make them not match with lambda later
        ## generate the final grid with NH-Poisson spikes locations
        ## I want to keep the naming of the variables used to generate final data the same as in other sections, so that the analysis part of the code can be recycled 
        
        spikes.index = which(grid %in% spikes.loc)
        # Generate the polynomial
        degree = length(poly.coefs) - 1
        smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
        ngrid = length(grid)
        y = smooth+rnorm(ngrid,0,noise.sd)
        height = rnorm(length(spikes.loc), height.mean, height.sd)
        y[spikes.index] = y[spikes.index] + height
        data = cbind.data.frame(y, grid)
        p = NA
        # if(plot == "yes" & is.na(arg.others)){
        #   layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        #   par(bg = "lightyellow", mar=c(1,1,1,1), oma = c(0,0,0,0), pin = c(6.7,3.1))
        #   #plot(grid,smooth, type = "l", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "")
        #   plot(grid,smooth, type = "l", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "")
        #   title(sub = "Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
        #   points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        #   #plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        #   plot(data$grid, data$y, type = "p", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        #   title(main = "Data: Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
        #   lines(grid, spikes.loc.param(grid), col = "green") #this is lambda
        # }
        # if(plot == "yes" & !is.na(arg.others)){
        #   layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        #   par(bg = "lightyellow", mar=c(1,1,1,1), oma = c(0,0,0,0), pin = c(6.7,3.1))
        #   #plot(grid,smooth, type = "l", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "")
        #   plot(grid,smooth, type = "l", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "")
        #   title(sub = "Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
        #   points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        #   #plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - height.mean - 4*height.sd, max(smooth)+height.mean + 4*height.sd), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        #   plot(data$grid, data$y, type = "p", ylim = c(1.5*min(y)-0.5*max(y), 1.5*max(y)-0.5*min(y)), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        #   title(main = "Data: Curves + Non-HMPP Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
        #   lines(grid, spikes.loc.param(grid, arg.others), col = "green") #this is lambda
        # }
        if(plot == "yes" & is.na(arg.others)){
          jBrewColors <- brewer.pal(n = 8, name = "Dark2")
          f <- list(
            family = "Courier New, monospace",
            size = 18,
            color = jBrewColors[1]
          )
          
          p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
            add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines", 
                      line = list(color = jBrewColors[2], size = 3))%>%
            add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                      mode = 'markers', 
                      marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                      mode = 'markers', 
                      marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid,y = ~spikes.loc.param(grid), name = 'mean function',
                      mode = 'lines',
                      line = list(color = jBrewColors[5], size = 3))%>%
            layout(xaxis = list(title = "Grid",titlefont = f),
                   yaxis = list(title = "Spiky Data",titlefont = f))
        }
        if(plot == "yes" & !is.na(arg.others)){
          jBrewColors <- brewer.pal(n = 8, name = "Dark2")
          f <- list(
            family = "Courier New, monospace",
            size = 18,
            color = jBrewColors[1]
          )
          
          p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
            add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines", 
                      line = list(color = jBrewColors[2], size = 3))%>%
            add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                      mode = 'markers', 
                      marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                      mode = 'markers', 
                      marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid,y = ~sapply(grid,spikes.loc.param, arg.others), name = 'mean function',
                      mode = 'lines',
                      line = list(color = jBrewColors[5], size = 3))%>%
            layout(xaxis = list(title = "Grid",titlefont = f),
                   yaxis = list(title = "Spiky Data",titlefont = f))
          
        }
        output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height,plot = p)
        return(output)
      }
    }
  }
}


#spikes.data = generate.data(range = c(0,10),by = 0.01,poly.coefs = c(1,3,-1,-0.2,0.03),nspikes = 15,spikes.loc.dist = "homopp", spikes.loc.param = c(5,2),spikes.dist = "nonhomopp",spikes.param = c(15,1),noise.sd = 2,plot = "yes")

# The second function is one that we use to separate spikes from smooth curve. Right now, we assume the distribution of white noise
# and also, the distribution of spike heights are normal. The main procedure is 
# (1) to fit an initial spline estimates using smoothing splines
# (2) then under some arbitrary threshold, perform a rough initial separation. feed this initial separation result to EM algorithm
# (3) after EM, we need to select a threshold p* to classify the spikes. In this step, use prior knowledge of the spike locations.
# This second function will contain multiple sub-functions corresponding to the steps above.
## The first sub-function does (1)
init = function(data=NA, range, norder= 5, nbasis=NA, Lcoef = c(0,0,1), lambda = NA,alpha.prior = 0.4, smooth.true = NA,spikes.flag = NA, plot = TRUE){
  # data: a data frame that consists of the observed y and the grid over which it is observed. The first column must be y, and the second is grid.
  ## the data output by the function generate.data is already conforming.
  # range of spline basis functions
  # norder: the order of spline basis to use
  # nbasis: the number of spline bases to use
  # Lcoef: the coefficients for penalization terms in smoothing splines, defaulted to c(1,1). First element corresponds to 1st deriv, 2nd to 2nd. 
  ## Penalization of higher order can be done by adding elements to the vector Lcoef.
  # lambda: a grid of tuning parameters to be used, must be of length 10. The length of this grid significantly affects the computing time (unles I optimize the code a little more)
  # smooth.true: if given, this is the true underlying curve, used to compared with the fitted splines. Only possible in simulations.
  # spikes.flag: if provided, these points will be excluded from the smoothing procedure. Can be useful in iterated smoothing and EM.
  # nbins and qt (quantile) are used to correct the contaminated estimates. We will partition the x-axis into nbins bins, and 
  ## within each bin, retain the data below this quantile and fit a smoothing spline. The result will be matched to the orifinal fit 
  ## (using all data) in the non-spikes region.
  # VALUE: if smooth.true is given, the function returns a data frame that consists of the smooth estimates at each grid point, smooth true, and MSE
  ## otherwise, it returns only the smooth estimates.
  #data = spikes.data$data;range = c(0,1); norder = 4; nbasis = 300; smooth.true = spikes.data$smooth;Lcoef = c(1,1);nbins=40;qt=0.2;spikes.flag = NA
  if(!is.data.frame(data)){
    stop('data must be of class data.frame') 
  }
  if(dim(data)[2]>2){
    warning('data has more than 2 columns. Only the first two are used.')
  }
  if (dim(data)[2]<2){
    stop('data must have at least 2 columns. The first contains measurements and the second contains the grid over which measurements are taken.')
  }
  colnames(data) = c("y", "grid")
  ngrid = length(data$grid)
  if(is.na(norder)){
    stop('norder must be specified')
  }
  if(norder <= 0 | !is.numeric(norder)){
    stop('Invalid norder. Must be a positive number')
  }
  if(is.na(nbasis)){
    warning('nbasis is defaulted to ', ngrid + norder - 2, ' .')
    nbasis = (ngrid -1) + (norder - 1) 
  }
  if(any(!is.na(smooth.true)) & length(smooth.true) != length(data$grid)){stop('\'smooth.true\' must be of the same length as grid.')}
  spline.basis = create.bspline.basis(rangeval=range, nbasis=nbasis, norder = norder)  #create basis
  Lfd = vec2Lfd(Lcoef, range)
  if(any(is.na(spikes.flag)) || length(spikes.flag) == 0){
    warning('either spikes.flag is not provided or it contains NA value. The entire dataset will be used to fit the splines. \n')
    smoothingdata = data  #this is the data we use to fit the curve.
    #if spikes.flag is provided, we exclude the spikes.flag. This is useful when we perform iterated EM.
  }else {smoothingdata = data[-spikes.flag,]}
  if (any(is.na(lambda))){lambda = 1/10^(0:9)}
  nlambda = length(lambda)
  spline.res = vector("list",nlambda)
  spikes.flag = vector("list",nlambda)
  smooth.data = vector("list",nlambda)
  for (i in 1:nlambda){
    fdParobj = fdPar(spline.basis, Lfd, lambda[i])
    smooth.data.temp = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)  #smooth the data
    spline.res.temp = data$y-eval.fd(data$grid,smooth.data.temp$fd)
    sort.index = order(spline.res.temp, decreasing = FALSE)
    # diff = diff(sort(spline.res.temp, decreasing = FALSE))
    # if (ngrid > 200){
    #   breakpoint = which.min(abs(diff - quantile(diff, prob = 0.9999))) #more robust to outliers; find the nearest locations in diff that has jump approximately quantile(diff, prob = 0.95)  
    #   #only recommended when there's sufficiently many data points
    # }else{
    #   breakpoint = which.max(diff)  
    # }
    # if(breakpoint<(1-alpha.prior)*ngrid){spikes.flag[[i]] = NA} else {
    kmeans_result = kmeans(spline.res.temp,2)
    kmeans_spikes_id = which.max(kmeans_result$centers)
    if(mean(kmeans_result$cluster == kmeans_spikes_id) > alpha.prior){spikes.flag[[i]] = NA} else {
      #refit spline
      #spikes.flag[[i]] = sort.index[-(1:breakpoint)]
      spikes.flag[[i]] = which(kmeans_result$cluster == kmeans_spikes_id)
      smooth.data[[i]] = smooth.basis(argvals = data$grid[-spikes.flag[[i]]],y = data$y[-spikes.flag[[i]]], fdParobj = fdParobj)
      spline.res[[i]] = data$y-eval.fd(data$grid,smooth.data[[i]]$fd)
    }
  }
  lambda = lambda[!is.na(spikes.flag)]
  spline.res = spline.res[!is.na(spikes.flag)]
  smooth.data = smooth.data[!is.na(spikes.flag)]
  spikes.flag = spikes.flag[!is.na(spikes.flag)] #this line needs to be last of all three because spikes.flag plays a role in previous 2 lines
  ####find lambda through gcv (code from reimherr book):
  # cat(paste('fitting smoothing spline: calculating gcv lambda... \n'))
  # lam = 0:9
  # nlam = length(lam)
  # dfsave = rep(NA,nlam)
  # names(dfsave) = lam
  # gcvsave = dfsave
  # for (ilam in 1:nlam) {
  #   #cat(paste('lambda =',10^-lam[ilam],'\n'))
  #   lambda = 10^-lam[ilam]
  #   fdParobj = fdPar(spline.basis, Lfd, lambda)
  #   smoothlist = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)
  #   dfsave[ilam] = smoothlist$df
  #   gcvsave[ilam] = sum(smoothlist$gcv)
  # }
  # #plot(lam, gcvsave, type='b', lwd=2)
  # lambda = 10^(-lam[which.min(gcvsave)[1]])  #which.min(gcvsave)[1] because there might be more than 1 min
  # cat(paste('fitting smoothing spline with gcv lambda = ',lambda,'.\n'))
  # fdParobj = fdPar(spline.basis, Lfd, lambda = lambda)
  # smooth.data = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)  #smooth the data
  # 
  # # plot the smoothed data
  # if(plot == TRUE & all(!is.na(smooth.true))){
  #   dev.off()
  #   par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,4,0), pin = c(6.5,4.5))
  #   plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.1)
  #   lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  #   lines(smooth.data, lwd = 3, col = "red", lty = 2)
  # }
  # if(plot == TRUE & any(is.na(smooth.true))){
  #   dev.off()
  #   par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,3,0), pin = c(5.5,3.5))
  #   plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.1)
  #   lines(smooth.data, lwd = 3, col = "red", lty = 2)
  # }
  # 
  # spline.res.cont = data$y-eval.fd(data$grid,smooth.data$fd) #possibly contaminated residuals
  # spikes.flag = vector("list", 5)
  # spline.res = vector("list", 5)
  # prob = seq(0.7,1,length.out = 6)
  # for (i in (1:5)){
  #   threshold = quantile(spline.res.cont, prob = prob[i])
  #   spikes.flag.cand = which(spline.res.cont > threshold)
  #   smooth.data = smooth.basis(argvals = data$grid[-spikes.flag.cand],y = data$y[-spikes.flag.cand], fdParobj = fdParobj)
  #   spline.res[[i]] = data$y-eval.fd(data$grid,smooth.data$fd)
  #   spikes.flag[[i]] = which(spline.res[[i]] > quantile(spline.res[[i]], prob = prob[i]))
  # }  
  ####################
  # for (i in (1:5)){
  #   threshold = quantile(spline.res.cont, prob = prob[i])
  #   spikes.flag[[i]] = which(spline.res.cont > threshold)
  #   smooth.data = smooth.basis(argvals = data$grid[-spikes.flag[[i]]],y = data$y[-spikes.flag[[i]]], fdParobj = fdParobj)
  #   spline.res[[i]] = data$y-eval.fd(data$grid,smooth.data$fd)
  # }
  
  # ##Compute residuals. Then, instead of sequential spikes removal, we create a threshold that determines how many
  # ## points we want to flag as spikes. This threshold can be adaptive as follow. We order the residuals e_i to e(i), and compute the magnitude
  # ## of jumps in residuals, i.e. max h_i = max e(i+1) - e(i). Let's assume that max h_i = h_j = e(j+1)- e(j). We can classify e(j+1),...,e(n)
  # ## as spikes. But there's a downside, if spikes have high variance, then the max jumps might occur deep in the spikes region. We want to be
  # ## more agressive, and get more spikes in the initial classification. Thus instead of using max h_i, we can use the 90% quantile of the h_i.
  # ##Since the likelihood is not convex (even for Gaussian mixture), EM might converge to one of the local extrema. 
  # ## It's a good idea to have multiple initialization, and run EM on these initialization. Then pick the EM results with
  # ## the largest likelihood.
  # spline.res.cont = data$y-eval.fd(data$grid,smooth.data$fd) #possibly contaminated residuals
  # sort.index = order(spline.res.cont, decreasing = FALSE)
  # diff = diff(sort(spline.res.cont, decreasing = FALSE))
  # spikes.flag = vector("list", 5)
  # spline.res = vector("list", 5)
  # #breakpoint = which.max(diff)
  # prob = seq(0.75,1,length.out = 5)
  # for (i in (1:5)){
  # breakpoint = which.min(abs(diff - quantile(diff, prob = prob[i]))) #find the h_i near the 75% quantile
  # spikes.flag[[i]] = sort.index[-(1:breakpoint)] #under the assumption that spikes make the residuals larger  
  # smooth.data = smooth.basis(argvals = data$grid[-spikes.flag[[i]]],y = data$y[-spikes.flag[[i]]], fdParobj = fdParobj)
  # spline.res[[i]] = data$y-eval.fd(data$grid,smooth.data$fd)
  # }
  # # Or Use k-means to separate?
  # spline.res = data$y-eval.fd(data$grid,smooth.data$fd)
  # kmeans = kmeans(spline.res,2)
  # spikes.flag = which(kmeans$cluster == which.min(kmeans$size)) #what if nspikes = 0?
  # smooth.data = smooth.basis(argvals = data$grid[-spikes.flag],y = data$y[-spikes.flag], fdParobj = fdParobj)  #fine tune the estimation
  #                                                                                                             #without this step, we are using EM to classify bad estimates
  # spline.res = data$y-eval.fd(data$grid,smooth.data$fd)
  
  # find = sort.index[order(c(0,diff),decreasing=TRUE)[1]]
  # dev.off()
  # par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,3,0), pin = c(5.5,3.5))
  # plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.2)
  # lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  # lines(smooth.data, lwd = 3, col = "red", lty = 2)
  # points(data$grid[find], data$y[find],pch = 3, cex = 2, col = "red")
  # 
  # diff2 = c(0, diff(sort(data$y, decreasing = FALSE)))
  # plot(diff2)
  # breakpoint2 = max(diff2)
  # sort.index2 = order(data$y, decreasing = FALSE)
  # flag2 =sort.index2[-(1:breakpoint2)]
  # sum(spikes.data$spikes.index %in% flag2)
  # find2 = sort.index2[order(c(0,diff2),decreasing=TRUE)[1]]
  # dev.off()
  # par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,3,0), pin = c(5.5,3.5))
  # plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.2)
  # lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  # lines(smooth.data, lwd = 3, col = "red", lty = 2)
  # points(data$grid[find2], data$y[find2],pch = 3, cex = 2, col = "red")
  #  
  # dev.off()
  # par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,4,0), pin = c(6.5,4.5))
  # plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.2)
  # lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  # lines(smooth.data, lwd = 3, col = "red", lty = 2)
  # points(data$grid[spikes.data$spikes.index], data$y[spikes.data$spikes.index],pch = 3, cex = 1, col = "red")
  
  # #partition the grid into k smaller sections
  # k = nbins
  # endpoint = seq(0,1, length.out = k+1)
  # part.grid = vector("list", k)
  # part.index = vector("list", k)
  # part.grid[[1]] = data$grid[data$grid <= endpoint[2]]
  # part.index[[1]] = which(data$grid %in% part.grid[[1]])
  # for (i in (2:k)){
  # part.grid[[i]] = data$grid[data$grid <= endpoint[i+1] & data$grid > endpoint[i]]  
  # part.index[[i]] = which(data$grid %in% part.grid[[i]])
  # }
  # 
  # new.grid = vector("list",5)
  # new.index = vector("list",5)
  # # n = unlist(sapply(1:k, function(i){
  # #   length(part.grid[[i]])
  # # }))
  # # min.n = min(n)
  # 
  # #partition the range of y
  # for(j in (1:5)){
  # new.grid[[j]] = unlist(lapply(1:k, function(i){
  #    part.grid[[i]][data$y[part.index[[i]]]<quantile(data$y[part.index[[i]]],probs = qt*j)
  #                                 & data$y[part.index[[i]]]>=quantile(data$y[part.index[[i]]],probs = qt*j - qt)]
  #   #part.grid[[i]][order(data$y[part.index[[i]]], decreasing = FALSE)[(1:floor(min.n/5))+5*(j-1)]]
  # }))
  # new.index[[j]] = which(data$grid %in% new.grid[[j]])
  # }
  # # for(j in (1:5)){
  # # new.grid[[j]] = unlist(sapply(1:k, function(i){
  # #    part.grid[[i]][data$y[part.index[[i]]]<(min(data$y[part.index[[i]]])+2*j)
  # #                                 &data$y[part.index[[i]]]>(min(data$y[part.index[[i]]])+2*j-2)]
  # #   #part.grid[[i]][order(data$y[part.index[[i]]], decreasing = FALSE)[(1:floor(min.n/5))+5*(j-1)]]
  # # }))
  # # new.index[[j]] = which(data$grid %in% new.grid[[j]])
  # # }
  # # new.grid[[6]] = unlist(sapply(1:k, function(i){
  # #    part.grid[[i]][data$y[part.index[[i]]]>(min(data$y[part.index[[i]]])+10)]
  # # }))
  # # new.index[[6]] = which(data$grid %in% new.grid[[6]])
  # 
  #   
  # fdParobj = fdPar(spline.basis, Lfd, lambda = lambda)
  # smooth.data1 = smooth.basis(argvals = data$grid[new.index[[1]]],y = data$y[new.index[[1]]], fdParobj = fdParobj)  #smooth the data
  # smooth.data2 = smooth.basis(argvals = data$grid[new.index[[2]]],y = data$y[new.index[[2]]], fdParobj = fdParobj)  #smooth the data
  # smooth.data3 = smooth.basis(argvals = data$grid[new.index[[3]]],y = data$y[new.index[[3]]], fdParobj = fdParobj)  #smooth the data
  # smooth.data4 = smooth.basis(argvals = data$grid[new.index[[4]]],y = data$y[new.index[[4]]], fdParobj = fdParobj)  #smooth the data
  # smooth.data5 = smooth.basis(argvals = data$grid[new.index[[5]]],y = data$y[new.index[[5]]], fdParobj = fdParobj)  #smooth the data
  # dev.off()
  # par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,4,0), pin = c(6.5,4.5))
  # plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab = "", cex = 0.1)
  # lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  # lines(smooth.data1, lwd = 2, col = "red", lty = 2)
  # lines(smooth.data2, lwd = 2, col = "blue", lty = 2)
  # lines(smooth.data3, lwd = 2, col = "red", lty = 2)
  # lines(smooth.data4, lwd = 2, col = "blue", lty = 2)
  # lines(smooth.data5, lwd = 2, col = "red", lty = 2)
  # 
  # i1 = 1
  # plot(data$grid[new.index[[i1]]], data$y[new.index[[i1]]], main = "", xlab = "",ylab="", cex = 0.1)
  # points(data$grid[new.index[[i1]][which(new.index[[i1]]%in%spikes.data$spikes.index)]], data$y[new.index[[i1]][which(new.index[[i1]]%in%spikes.data$spikes.index)]], pch = 4, cex = 0.3, col = "red")
  # 
  # lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  # lines(smooth.data1, lwd = 3, col = "red", lty = 2)
  # 
  # fit = eval.fd(data$grid,smooth.data$fd)
  # fit1 = eval.fd(data$grid,smooth.data1$fd)
  # fit2 = eval.fd(data$grid,smooth.data2$fd)
  # fit3 = eval.fd(data$grid,smooth.data3$fd)
  # fit4 = eval.fd(data$grid,smooth.data4$fd)
  # fit5 = eval.fd(data$grid,smooth.data5$fd)
  # fit.comb = cbind(fit1,fit2, fit3, fit4)
  # fit.mean = apply(fit.comb, 1, mean)
  # plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.1)
  # lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
  # lines(smooth.data, lwd = 3, col = "red", lty = 2)
  # lines(data$grid,fit.mean + min(fit-fit.mean), lty = 2, col = "red", lwd = 3)
  # lines(data$grid,fit1+ min(fit-fit1), lty = 2, col = "blue", lwd = 3)
  
  return(list(smooth.data = smooth.data,spline.res = spline.res, spikes.flag = spikes.flag, lambda= lambda))
}


## The second sub-function uses EM to find MLE. For now, this applies when spikes sizes are normal and white noise is normal.
mle = function(data, spline.res, spikes.flag = NA, stop.crit = 1e-05, constraint = 0.05, alpha.prior = 0.4, VIOM = FALSE, MSOM = TRUE){
  #constraint: a lower bound on the variance of components to prevent degeneracy
  #alpha.prior: the prior on percentage of spikes. If EM classify more than alpha points as spikes, we reject this classification
  ## This parameter ensures that the code doesn't underestimate the smooth and (in the extreme case) classify every point as spikes
  #VIOM: variance inflated spikes
  #MSOM: mean shift spikes
  if(VIOM == TRUE & MSOM == TRUE){
    if(length(spikes.flag) == 0 || is.na(spikes.flag)){
      ngrid = length(data$grid)
      alpha.0 = 0
      sd.0 = sqrt(sum((spline.res)^2)/(length(spline.res)-1))  #we are not using the spikes.flag to estimate noise.sd here
      mu.h.0 = 0
      sigma.h.0 = constraint
    }else {
      ngrid = length(data$grid)
      alpha.0 = length(spikes.flag)/ngrid
      sd.0 = sqrt(sum((spline.res[-spikes.flag])^2)/(length(spline.res[-spikes.flag])-1))  #we are not using the spikes.flag to estimate noise.sd here
      mu.h.0 = mean(spline.res[spikes.flag])
      sigma.h.0 = sqrt(max(sum((spline.res[spikes.flag] - mu.h.0)^2)/length(spline.res[spikes.flag])-(sd.0^2), constraint))
    }
    
    EM = list(alpha = c(),sd = c(),mu.h = c(),sigma.h = c(), prob = c()) #inefficient storage scheme. Need review
    EM$alpha[1] = alpha.0
    EM$sd[1] = sd.0
    EM$mu.h[1] = mu.h.0
    EM$sigma.h[1] = sigma.h.0
    
    it = 1 #keep track of iteration
    test = 1 #measures the convergence of estimates, to be compared with stopping criteria
    crit = stop.crit  #stopping criteria for EM
    while(test > crit){
      cat("EM it = ", it, "\n")
      alpha = EM$alpha[it]
      sd = EM$sd[it]
      mu.h = EM$mu.h[it]
      sigma.h = EM$sigma.h[it]
      #E step:
      EM.prob = sapply(1:ngrid, function(i){
        component.1 = dnorm(spline.res[i],0, sd)*(1-alpha)  #joint density of data res.i and z.i1 (resi belongs to component 1)
        component.2 = dnorm(spline.res[i],mu.h, sqrt(sigma.h^2 + sd^2))*alpha
        prob.1 = component.1/(component.1+component.2)  #This is probability of residual belonging to N(0,sd) given observed data and current estimate of param
        return(c(prob.1, 1-prob.1))
      })
      EM$prob = EM.prob[2,]  #save the probability of belonging to spike component in the last iteration
      #EM$flag = which(EM.prob[2,]>0.6)
      #M step:
      EM$alpha[it+1] = sum(EM.prob[2,])/sum(EM.prob)
      EM$mu.h[it+1] = ifelse(sum(EM.prob[2,])>0,sum(spline.res*EM.prob[2,])/sum(EM.prob[2,]),0)
      #propose.sigma.h = sqrt(sum(EM.prob[2,]*(spline.res-EM$mu.h[it+1])^2)/sum(EM.prob[2,]) - (EM$sd[it]^2))
      propose.sigma2.h = ifelse(sum(EM.prob[2,]) >0,sum(EM.prob[2,]*(spline.res-EM$mu.h[it])^2)/sum(EM.prob[2,]) - (EM$sd[it]^2), constraint)
      EM$sigma.h[it+1] = ifelse(propose.sigma2.h<constraint, sqrt(constraint), sqrt(propose.sigma2.h)) 
      
      # #solve cubic polynomial in noise.sd^2
      c3 = -sum(EM.prob) #coef of noise.sd^6
      c2 = sum(EM.prob[1,]*(spline.res^2)) - 2*(EM$sigma.h[it+1]^2)*sum(EM.prob[1,]) + sum(EM.prob[2,]*(spline.res - EM$mu.h[it+1])^2) - (EM$sigma.h[it+1]^2)*sum(EM.prob[2,])
      c1 = 2*(EM$sigma.h[it+1]^2)*sum(EM.prob[1,]*(spline.res^2)) - (EM$sigma.h[it+1]^4)*sum(EM.prob[1,])
      c0 = (EM$sigma.h[it+1]^4)*sum(EM.prob[1,]*(spline.res^2))
      # a2 = c2/c3
      # a1 = c1/c3
      # a0 = c0/c3
      # 
      # Q = (3*a1-a2^2)/9
      # R = (9*a2*a1 - 27*a0-2*a2^3)/54
      # D = Q^3 + R^2
      # if(D > 0){
      #   S = (R+sqrt(D))^(1/3)
      #   T = ifelse(is.nan((R-sqrt(D))^(1/3)),0,(R-sqrt(D))^(1/3))
      #   roots = -a2/3 + (S + T)   
      # }
      # if(D<0){
      #   argcos = acos(R/sqrt(-Q^3))
      #   roots = c(2*sqrt(-Q)*cos(argcos/3)-a2/3,2*sqrt(-Q)*cos((argcos+2*pi)/3)-a2/3,2*sqrt(-Q)*cos((argcos+4*pi)/3)-a2/3)
      # }
      # if(D == 0){
      #   S = R^(1/3)
      #   roots = c(-a2/3 + 2*S,-a2/3 - S)
      # }
      comp.roots = polyroot(c(c0,c1,c2,c3))
      real.roots = Re(comp.roots)[abs(Im(comp.roots)) < 1e-6]
      #EM$sd[it+1] = ifelse(any(roots>1e-05),sqrt(roots[roots>1e-05]),constraint)
      EM$sd[it+1] = sqrt(real.roots[real.roots>0])
      
      test = sum(sapply(1:4, function(k) {
        abs(EM[[k]][[it+1]] - EM[[k]][[it]])
      }))
      #cat("it = ", it, " test = ", test, "\n")
      it = it+1
    }
    # calculate predictive loglikelihood: expectation of the loglikelihood, taken over membership vector z;
    # doesn't depend on classification results, we think of this as a function of membership probabilities
    llh.pred = sum(sapply(1:ngrid, function(i){
      ifelse((1-EM$prob[i])>0,(1-EM$prob[i])*(log((1-EM$alpha[it])*dnorm(spline.res[i], 0, EM$sd[it]))),0) + ifelse(EM$prob[i] > 0,EM$prob[i]*(log(EM$alpha[it]*dnorm(spline.res[i], EM$mu.h[it], sqrt(EM$sd[it]^2 + EM$sigma.h[it]^2)))),0)
    })
    )
    threshold = seq(0.5,1,length.out = 6)
    spikes.EM.cand = vector("list", 5)
    loglike.cand = rep(NA, 5)
    check = rep(TRUE,5) #a candidate is admitted if it contains less than 80% of data points
    for (i in (1:5)){
      spikes.EM.cand[[i]] = which(EM$prob>threshold[i])
      loglike.cand[i] = sum(log(dnorm(spline.res[spikes.EM.cand[[i]]],EM$mu.h[it], sqrt((EM$sigma.h[it])^2 + (EM$sd[it])^2))))+sum(log(dnorm(spline.res[-spikes.EM.cand[[i]]],0, EM$sd[it])))
      if (length(spikes.EM.cand[[i]]) > round(alpha.prior*ngrid)) {check = FALSE}
    }
    if(any(check)){
      best = which.max(loglike.cand[check])
      spikes.EM = spikes.EM.cand[[best]]
      loglike = loglike.cand[best]  
    }
    else {
      spikes.EM = c()
      loglike = sum(log(dnorm(spline.res,0, EM$sd[it])))
    }
    
    #save final result
    # alpha.EM = EM$alpha[it]
    # height.EM = EM$mu.h[it]
    # sd.EM = EM$sd[it]
    # prob.EM = EM$prob
    return(list(loglike = loglike,llh.pred=llh.pred,param = c(alpha.EM = EM$alpha[it], height.EM = EM$mu.h[it], sigma.h.EM = EM$sigma.h[it],sd.EM = EM$sd[it]),spikes.EM = spikes.EM,prob.EM = EM$prob))
  }
  if (VIOM == FALSE & MSOM == TRUE){
    if(length(spikes.flag) == 0 || is.na(spikes.flag)){
      ngrid = length(data$grid)
      alpha.0 = 0
      sd.0 = sqrt(sum((spline.res)^2)/(length(spline.res)-1))  #we are not using the spikes.flag to estimate noise.sd here
      mu.h.0 = 0
      #sigma.h.0 = sigma.h.0 = sqrt(max(sum((spline.res[spikes.flag] - mu.h.0)^2)/length(spline.res[spikes.flag])-(sd.0^2), constraint*sd.0))
    }else {
      ngrid = length(data$grid)
      alpha.0 = length(spikes.flag)/ngrid
      mu.h.0 = mean(spline.res[spikes.flag])
      sd.0 = sqrt((sum((spline.res[-spikes.flag])^2) + sum((spline.res[spikes.flag]-mu.h.0)^2))/ngrid)
      #sigma.h.0 = sqrt(max(sum((spline.res[spikes.flag] - mu.h.0)^2)/length(spline.res[spikes.flag])-(sd.0^2), constraint*sd.0))
    }
    
    EM = list(alpha = c(),sd = c(),mu.h = c(),
              #sigma.h = c(), 
              prob = c()) #inefficient storage scheme. Need review
    EM$alpha[1] = alpha.0
    EM$sd[1] = sd.0
    EM$mu.h[1] = mu.h.0
    #EM$sigma.h[1] = sigma.h.0
    
    it = 1 #keep track of iteration
    test = 1 #measures the convergence of estimates, to be compared with stopping criteria
    crit = stop.crit  #stopping criteria for EM
    while(test > crit){
      alpha = EM$alpha[it]
      sd = EM$sd[it]
      mu.h = EM$mu.h[it]
      #sigma.h = EM$sigma.h[it]
      #E step:
      EM.prob = sapply(1:ngrid, function(i){
        component.1 = dnorm(spline.res[i],0, sd)*(1-alpha)  #joint density of data res.i and z.i1 (resi belongs to component 1)
        component.2 = dnorm(spline.res[i],mu.h, sd)*alpha
        prob.1 = component.1/(component.1+component.2)  #This is probability of residual belonging to N(0,sd) given observed data and current estimate of param
        return(c(prob.1, 1-prob.1))
      })
      EM$prob = EM.prob[2,]  #save the probability of belonging to spike component in the last iteration
      #EM$flag = which(EM.prob[2,]>0.6)
      #M step:
      EM$alpha[it+1] = sum(EM.prob[2,])/sum(EM.prob)
      EM$mu.h[it+1] = ifelse(sum(EM.prob[2,])>0,sum(spline.res*EM.prob[2,])/sum(EM.prob[2,]),0)
      EM$sd[it+1] = sqrt((sum((spline.res^2)*EM.prob[1,]) + sum(((spline.res-EM$mu.h[it+1])^2)*EM.prob[2,]))/sum(EM.prob))
      
      test = sum(sapply(1:3, function(k) {
        abs(EM[[k]][[it+1]] - EM[[k]][[it]])
      }))
      #cat("it = ", it, " test = ", test, "\n")
      it = it+1
    }
    # calculate predictive loglikelihood: expectation of the loglikelihood, taken over membership vector z;
    # doesn't depend on classification results, we think of this as a function of membership probabilities
    llh.pred = sum(sapply(1:ngrid, function(i){
      ifelse((1-EM$prob[i])>0,(1-EM$prob[i])*(log((1-EM$alpha[it])*dnorm(spline.res[i], 0, EM$sd[it]))),0) + ifelse(EM$prob[i] > 0,EM$prob[i]*(log(EM$alpha[it]*dnorm(spline.res[i], EM$mu.h[it], EM$sd[it]))),0)
    })
    )
    threshold = seq(0.5,1,length.out = 6)
    spikes.EM.cand = vector("list", 5)
    loglike.cand = rep(NA, 5)
    check = rep(TRUE,5) #a candidate is admitted if it contains less than 30% of data points
    for (i in (1:5)){
      spikes.EM.cand[[i]] = which(EM$prob>threshold[i])
      loglike.cand[i] = sum(log(dnorm(spline.res[spikes.EM.cand[[i]]],EM$mu.h[it], EM$sd[it])))+sum(log(dnorm(spline.res[-spikes.EM.cand[[i]]],0, EM$sd[it])))
      if (length(spikes.EM.cand[[i]]) > round(alpha.prior*ngrid)) {check = FALSE}
    }
    if(any(check)){
      best = which.max(loglike.cand[check])
      spikes.EM = spikes.EM.cand[[best]]
      loglike = loglike.cand[best]  
    }
    else {
      spikes.EM = c()
      loglike = sum(log(dnorm(spline.res,0, EM$sd[it])))
    }
    
    #save final result
    # alpha.EM = EM$alpha[it]
    # height.EM = EM$mu.h[it]
    # sd.EM = EM$sd[it]
    # prob.EM = EM$prob
    return(list(loglike = loglike,llh.pred=llh.pred,param = c(alpha.EM = EM$alpha[it], height.EM = EM$mu.h[it],sd.EM = EM$sd[it]),spikes.EM = spikes.EM,prob.EM = EM$prob))
  }
}

## This is the main function that puts together the procedure. It uses both the init() and mle() functions
classify = function(data, range, norder = 5, nbasis=NA,lambda = NA, Lcoef = c(0,0,1), smooth.true = NA, constraint = 0.5, alpha.prior= 0.3, MSOM = TRUE, VIOM = FALSE,plot = TRUE){
  # data: is the usual data frame that contains the grid and y values
  # smooth.true: if provided (only possible in simulation setting), plot the estimate next to the true smooth function
  ###the function returns the best iterated mle estimates. That is, in each iteration, we select the best initialization with the
  ###highest predictive soft likelihood (treated as a function of membership vector z in EM). Then, classify spikes such that the 
  ###classification maximizes the hard likelihood. 
  # constraint, alpha.prior: these are passed onto mle() function
  if(!is.data.frame(data)){
    stop('data must be of class data.frame') 
  }
  if(dim(data)[2]>2){
    warning('data has more than 2 columns. Only the first two are used.')
  }
  if (dim(data)[2]<2){
    stop('data must have at least 2 columns. The first contains measurements and the second contains the grid over which measurements are taken.')
  }
  colnames(data) = c("y", "grid")
  ngrid = length(data$grid)
  if(is.na(norder)){
    stop('norder must be specified')
  }
  if(norder <= 0 | !is.numeric(norder)){
    stop('Invalid norder. Must be a positive number')
  }
  if(is.na(nbasis)){
    warning('nbasis is defaulted to ', ngrid + norder - 2, ' .')
    nbasis = (ngrid -1) + (norder - 1) 
  }
  if(any(!is.na(smooth.true)) & length(smooth.true) != length(data$grid)){stop('\'smooth.true\' must be of the same length as grid.')}
  flag.new = NA
  flag.old = vector("list")
  it = 1
  check = TRUE
  smooth.fit = NA
  while (check){
    flag.old[[it]] = flag.new
    #initialize spikes flag
    init1 = init(data = data,range = range, norder = norder, lambda = lambda,nbasis = nbasis, Lcoef = Lcoef, alpha.prior = alpha.prior, smooth.true = smooth.true, spikes.flag = flag.new, plot = plot)
    ninit = length(init1$spikes.flag)
    if (ninit == 0) {
      cat("cat1 \t")
      smooth.data = fit(data,range = range, norder = norder, nbasis = nbasis, lambda = lambda,plot = FALSE) #no smooth.true provide means calculating smoothdata w gcv lambda
      res = data$y - eval.fd(data$grid,smooth.data$fd)
      best = mle(data = data, spline.res = res, stop.crit = 1e-03, constraint = constraint, alpha.prior= alpha.prior, MSOM= MSOM, VIOM = VIOM)
      flag.new = best$spikes.EM
      lamb = NA
    }else{
      cat("cat2 \t")
      em = vector("list",ninit)
      llh.pred = vector("list",ninit)
      #Run MLE with different initialization
      for(i in 1:ninit){
        em[[i]] = mle(data = data, spline.res = init1$spline.res[[i]], spikes.flag = init1$spikes.flag[[i]], stop.crit = 1e-03, constraint = constraint, alpha.prior= alpha.prior, MSOM= MSOM, VIOM = VIOM)
        llh.pred[i] = em[[i]]$llh.pred
        cat('llh.pred = ', em[[i]]$llh.pred,'\t')
      }
      best.ind = which.max(llh.pred)
      cat('best.ind = ', best.ind, '\n')
      flag.new = em[[best.ind]]$spikes.EM
      best = em[[best.ind]]
      smooth.fit = init1$smooth.data[[best.ind]]
      lamb = init1$lambda[best.ind]
    }
    cat('length(flag) ', length(flag.new), ' it = ', it,'\n')
    if (length(flag.new) == 0) check = FALSE  #this is to prevent looping, as it goes back to 1st iteration
    for (i in (2:it)){ #this is also to prevent looping
      if (it ==1) break
      if (all(flag.new == flag.old[[i]])) {
        check = FALSE
        break
      }
    }
    it = it + 1
  }
  
  # while (any(is.na(flag.new)) || any(!(flag.new %in% flag.old)) || any(!(flag.old %in% flag.new))){
  #   if(!any(is.na(flag.new))){
  #     flag.old = flag.new
  #   }
  #   #initialize spikes flag
  #   init = init(data = data,range = range, norder = norder, nbasis = nbasis, Lcoef = Lcoef, smooth.true = smooth.true, spikes.flag = flag.new, plot = plot)
  #   #Run MLE 5 times with 5 different initialization
  #   mle1 = mle(data = data, spline.res = init$spline.res[[1]], spikes.flag = init$spikes.flag[[1]], stop.crit = 1e-05, constraint = constraint, alpha.prior= alpha.prior)
  #   #data = data; spline.res = init$spline.res[[1]]; spikes.flag = init$spikes.flag[[1]]; stop.crit = 1e-05
  #   mle2 = mle(data = data, spline.res = init$spline.res[[2]], spikes.flag = init$spikes.flag[[2]], stop.crit = 1e-05, constraint = constraint, alpha.prior= alpha.prior)
  #   mle3 = mle(data = data, spline.res = init$spline.res[[3]], spikes.flag = init$spikes.flag[[3]], stop.crit = 1e-05, constraint = constraint, alpha.prior= alpha.prior)
  #   mle4 = mle(data = data, spline.res = init$spline.res[[4]], spikes.flag = init$spikes.flag[[4]], stop.crit = 1e-05, constraint = constraint, alpha.prior= alpha.prior)
  #   mle5 = mle(data = data, spline.res = init$spline.res[[5]], spikes.flag = init$spikes.flag[[5]], stop.crit = 1e-05, constraint = constraint, alpha.prior= alpha.prior)
  #   best.ind = which.max(c(mle1$llh.pred,mle2$llh.pred,mle3$llh.pred,mle4$llh.pred,mle5$llh.pred))
  #   flag.new = list(mle1$spikes.EM,mle2$spikes.EM,mle3$spikes.EM,mle4$spikes.EM,mle5$spikes.EM)[[best.ind]]
  #   cat('length(flag) ', length(flag.new), ' it = ', it,'\n')
  #   if (length(flag.new) == 0) break  #this is to prevent looping, as it goes back to 1st iteration
  #   it = it + 1
  # }
  
  return(list(smooth.fit = smooth.fit, EM.result = best, lambda = lamb))
}

# This function provide the final fit based on the classification from the procedure above. It also calculates some goodness of fit diagnostics.
fit = function(data, range, norder = 5, nbasis=NA, Lcoef = c(0,0,1), smooth.true = NA, spikes.flag = NA, lambda = NA, plot = FALSE){
  # if smooth.ind is provided (only possible in simulations), then return the smooth residuals.
  if(!is.data.frame(data)){
    stop('data must be of class data.frame') 
  }
  if(dim(data)[2]>2){
    warning('data has more than 2 columns. Only the first two are used.')
  }
  if (dim(data)[2]<2){
    stop('data must have at least 2 columns. The first contains measurements and the second contains the grid over which measurements are taken.')
  }
  colnames(data) = c("y", "grid")
  ngrid = length(data$grid)
  if(is.na(norder)){
    stop('norder must be specified')
  }
  if(norder <= 0 | !is.numeric(norder)){
    stop('Invalid norder. Must be a positive number')
  }
  if(is.na(nbasis)){
    warning('nbasis is defaulted to ', ngrid + norder - 2, ' .')
    nbasis = (ngrid -1) + (norder - 1) 
  }
  if(any(!is.na(smooth.true)) & length(smooth.true) != length(data$grid)){stop('\'smooth.true\' must be of the same length as grid.')}
  spline.basis = create.bspline.basis(rangeval=range, nbasis=nbasis, norder = norder)  #create basis
  Lfd = vec2Lfd(Lcoef, range)
  if(any(is.na(spikes.flag)) || length(spikes.flag)== 0){
    warning('either spikes.flag is not provided or it contains NA value. The entire dataset will be used to fit the splines. \n')
    smoothingdata = data  #this is the data we use to fit the curve.
    #if spikes.flag is provided, we exclude the spikes.flag
  }else {smoothingdata = data[-spikes.flag,]}
  if (is.na(lambda)){
    cat(paste('fitting smoothing spline: calculating gcv lambda... \n'))
    lam = 0:9
    nlam = length(lam)
    dfsave = rep(NA,nlam)
    names(dfsave) = lam
    gcvsave = dfsave
    for (ilam in 1:nlam) {
      #cat(paste('lambda =',10^-lam[ilam],'\n'))
      lambda = 10^-lam[ilam]
      fdParobj = fdPar(spline.basis, Lfd, lambda)
      smoothlist = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)
      dfsave[ilam] = smoothlist$df
      gcvsave[ilam] = sum(smoothlist$gcv)
    }
    #plot(lam, gcvsave, type='b', lwd=2)
    lambda = 10^(-lam[which.min(gcvsave)[1]])  #which.min(gcvsave)[1] because there might be more than 1 min
    cat(paste('fitting smoothing spline with gcv lambda = ',lambda,'.\n'))
  }else{
    cat(paste('fitting smoothing spline: calculating gcv lambda... \n'))
    nlam = length(lambda)
    dfsave = rep(NA,nlam)
    gcvsave = dfsave
    for (ilam in 1:nlam) {
      fdParobj = fdPar(spline.basis, Lfd, lambda[ilam])  
      smoothlist = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)
      dfsave[ilam] = smoothlist$df
      gcvsave[ilam] = sum(smoothlist$gcv)
    }
    lambda = lambda[which.min(gcvsave)[1]]  #which.min(gcvsave)[1] because there might be more than 1 min
    cat(paste('fitting smoothing spline with gcv lambda = ',lambda,'.\n'))
  }
  fdParobj = fdPar(spline.basis, Lfd, lambda = lambda)
  smooth.data = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)  #smooth the data
  # plot the smoothed data
  if(plot == TRUE & all(!is.na(smooth.true))){
    dev.off()
    par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,4,0), pin = c(6.5,4.5))
    plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.1)
    lines(data$grid,smooth.true, lty = 2, col = "green", lwd = 3)
    lines(smooth.data, lwd = 3, col = "red", lty = 2)
  }
  if(plot == TRUE & any(is.na(smooth.true))){
    dev.off()
    par(bg = "lightyellow", mar = c(0,0,3,0),oma = c(2,0,3,0), pin = c(5.5,3.5))
    plot(data$grid, data$y, main = "Spline fit", xlab = "",ylab="", cex = 0.1)
    lines(smooth.data, lwd = 3, col = "red", lty = 2)
  }
  if(all(!is.na(smooth.true))){
    smooth.res = smooth.true-eval.fd(data$grid,smooth.data$fd)  
    return(list(l1 = mean(abs(smooth.res)), l2 = mean(smooth.res^2), linf = max(abs(smooth.res)), smooth.res = smooth.res))
  }else{
    #return(list(smooth.data = smooth.data, lambda = lambda))
    return(smooth.data)
  }
}