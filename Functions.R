require(fda)
require(plotly)
require(RColorBrewer)
# This file contain R code for the functions needed in the smoothEM project
# The first function that we need is that we use to generate data
# This function return a list of elements, notably among which are:
## spikes.index: a vector of spikes index. Each element takes value between 1 and length(grid), where grid is created from 'range' and 'by' (and sometimes from spikes generated as well.)
## spikes.loc: the actual location of spikes. Each element takes value between range[1] and range[2]
## smooth: the true smooth part evaluated at grid
## data: a data frame. y is the actual data we want to analyze. grid is, well, grid.
generate.data = function(range, by = 0.01,poly.coefs = NA, smooth,nspikes=NA,spikes.loc.dist="homopp",spikes.loc.param=NA, arg.others = NA,spikes.dist="fixed",spikes.param=NA, noise.sd,plot = FALSE){
  # Caution: if generating spikes according to homopp or nonhomopp, we generate the location of spikes, then move these locations to the nearest grid points
  ## If there are more than 1 spikes between 2 grid points, they are merged. Thus, the smaller the grid, the more accurate the number of spikes is
  # range: is the domain of the polynomials
  # by: the binning over the range, default to 0.01
  # poly.coefs is the coefficients of underlying polynomial curves, of order from low to high
  ## that is: poly.coefs[1] is the coefficient of degree zero term.
  # spikes.loc.dist: distribution of the spikes location, default to "homopp"
  ## if spikes.dist == fixed, that is height of spikes is constant, spikes.loc.dist can be "uniform", "homopp", "nonhomopp"
  ## if spikes.dist is random, spikes.loc.dist must be either "homopp" or "nonhomopp"
  # spikes.loc.param:
  ## in HomoPP, this is the rate of interarrival time; if not specified, must specify 'nspikes' and the code will give approximately 'nspikes' spikes.
  ## in NonHomoPP, this is the mean function lambda(x). note that the mean function must be of the same length as the grid generated by "range" and "by"
  ## in Normal: this is a vector c(mean,sd) of the normal distribution
  # arg.others:
  ## if spikes.loc.param is a function, arg.others are extra arguments to be fed into the function. If there are more than 1 extra arguments, must be a vector.
  # spikes.dist: distribution of the spikes height, default to 'fixed', ie constant
  ## if spikes.loc.dist == "homopp", spikes.dist can be "fixed", "normal", "chisq", "ncchisq", or a function that generate spikes height.
  ### If a function, the first argument must be the approximate number of spikes user wants to generate. Other arguments necessary must be contained in spikes.param in correct order.
  ## if spikes.loc.dist != "homopp", spikes.dist must be "fixed". We might change this later.
  # spikes.param: the parameters of the distribution for spikes height.
  ## if spikes.dist == "fixed", spikes.param is the constant height of spikes
  ## if spikes.dist is random, spikes.param is the (vector of) parameter(s) for 'spikes.dist'
  # noise.sd: the sd of white noise
  # plot: if 'yes', plot the generated data
  # Caution2: if spikes.dist == fixed, i.e. spikes height is a constant, the function returns constant height.
  # If spikes.dist != fixed, the function returns a vector of height values for each observations. The code for spikes.dist != fixed is not polished
  # and not used in our smoothEM paper.
  if(spikes.dist == "fixed"){ #Case 1: fixed height for spikes
    if(length(spikes.param)>1) warning('\'spikes.param\' has multiple elements. Only the first element will be used.')
    height = spikes.param[1]
    if(spikes.loc.dist == "uniform"){
      # check range
      if (diff(range) <= 0) stop('Invalid \'range\'.')
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes location
      spikes.loc = runif(nspikes, min = range[1], max = range[2])    #spikes location/index in grid rather
      for (i in (1:length(spikes.loc))){ #This function merge the spike locations with their nearest grids. If there're more than 2 spikes
        # between 2 consecutive grid points, they are merged.
        spikes.loc[i] = grid[which.min(abs(grid-spikes.loc[i]))]
      }
      spikes.loc=unique(spikes.loc)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial part
      if (missing(smooth)){
        degree = length(poly.coefs) - 1
        smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      }
      # Generate data
      ngrid = length(grid)
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      # Plot data if user opts to
      p = NA
      if(plot == "yes"){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )
        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "markers",
                    marker = list(color = jBrewColors[2], size = 5, symbol = 'o'))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers',
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 5))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers',
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 5))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
        p
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc,smooth = smooth, data = data, height = height, plot = p)
      return(output)
    }
    if(spikes.loc.dist == "homopp"){
      # Spikes from Homogeneous Poisson Process
      if(any(is.na(spikes.loc.param))){
        if (is.na(nspikes)) stop('Must specify either \'spikes.loc.param\' or \'nspikes\'.')
        warning('Approximately ', nspikes, ' spikes will be generated.')
        want.nspikes = nspikes  #approximately how many spikes we want to generate
        pp.rate = want.nspikes/diff(range)  ##higher rate leads to higher nspikes
      }
      if(all(!is.na(spikes.loc.param))){
        if (spikes.loc.param[1] <= 0) stop('Invalid \'spikes.loc.param\'.')
        if (length(spikes.loc.param) > 1) warning('\'spikes.loc.param\' has multiple elements. Only the first element will be used')
        pp.rate = spikes.loc.param[1]
      }
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes
      ## let numerator be approximate the number of spikes we want
      ## denominator is the range of x axis
      want.nspikes = pp.rate*diff(range) #in case spikes.loc.param is specified
      wait = rexp(want.nspikes + 100, rate = pp.rate)  #wait times of homogeneous poisson process are independent exponential
      ## we are generating more than we need
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes
      spikes.loc= cumsum(wait)
      spikes.loc = spikes.loc - (min(spikes.loc) - range[1])  ##transform spikes location to have them in the range
      spikes.loc = spikes.loc[spikes.loc < range[2]]  ##remove those beyond the range
      for (i in (1:length(spikes.loc))){ #This function merge the spike locations with their nearest grids. If there're more than 2 spikes
        # between 2 consecutive grid points, they are merged.
        spikes.loc[i] = grid[which.min(abs(grid-spikes.loc[i]))]
      }
      spikes.loc=unique(spikes.loc)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial
      if (missing(smooth)){
        degree = length(poly.coefs) - 1
        smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      }
      # Generate data
      ngrid = length(grid)
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      if(plot == "yes"){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )

        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "markers",
                    marker = list(color = jBrewColors[2], size = 5, symbol = 'o'))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers',
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 5))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers',
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 5))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height, plot = p)
      return(output)
    }
    if(spikes.loc.dist == "nonhomopp"){
      ## Using thinning method by Lewis and Shedler (1978)
      ## Doesn't require integration of the mean function, so it's good even for complicated mean functions that are hard to integrate
      ## However we do need to be able to evaluate the mean function at the grid
      ## Generate non-homogeneous Poisson process (NHPP)
      ## First, generate a homogeneous Poisson process with rate lambda_h = max(lambda), where h stands for homogeneous, the non-homogeneous is suppressed from lambda_nh to be just lambda
      grid = seq(range[1],range[2],by = by)
      if (!is.function(spikes.loc.param)) stop('spikes.loc.param must be a function.')
      if (any(is.na(arg.others))) lambda = sapply(grid,spikes.loc.param) else lambda=sapply(grid,spikes.loc.param,arg.others)
      lambda.h = max(lambda)
      wait.h = rexp(100000, rate = lambda.h)  ## wait times of homogeneous poisson process are independent exponential
      ## we are generating 10000 wait times, we can adjust this number later
      spikes.h.loc = cumsum(wait.h) ## spikes.h.loc are spikes from homogeneous Poisson, candidates for deletion (or NHPP, depending on how you view it)
      for (i in (1:length(spikes.h.loc))){
        spikes.h.loc[i] = grid[which.min(abs(grid-spikes.h.loc[i]))]
      }
      spikes.h.loc=unique(spikes.h.loc)
      spikes.h.index = which(grid %in% spikes.h.loc)
      ## thinning requires knowing the ratio of lambda/lambda.h
      ## If if turns out that lambda[spikes.h] > lambda.h, that's okay, it means that we will not delete spikes at these locations.
      nspikes.h = length(spikes.h.loc)
      spikes.loc = c()
      i = 1
      while(i<=nspikes.h){
        if (runif(1,0,1)< (lambda[spikes.h.index[i]]/lambda.h)) spikes.loc = c(spikes.loc,spikes.h.loc[i])
        i = i+1
      }
      spikes.loc = spikes.loc[spikes.loc >= range[1]]  #we don't transform the spikes.loc here, because if we do so it will mismatch with the mean function
      spikes.loc = spikes.loc[spikes.loc <= range[2]]
      ## note that we are not doing (spikes.loc - range[1])/range[2], which will shrink the locations and make them not match with lambda later
      ## generate the final grid with NH-Poisson spikes locations
      ## I want to keep the naming of the variables used to generate final data the same as in other sections, so that the analysis part of the code can be recycled
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial
      if (missing(smooth)){
        degree = length(poly.coefs) - 1
        smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      }
      ngrid = length(grid)
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      p = NA
      if(plot == "yes" & any(is.na(arg.others))){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )

        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines",
                    line = list(color = jBrewColors[2], size = 3))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers',
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers',
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid,y = ~spikes.loc.param(grid), name = 'mean function',
                    mode = 'lines',
                    line = list(color = jBrewColors[5], size = 3))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))
      }
      if(plot == "yes" & all(!is.na(arg.others))){
        jBrewColors <- brewer.pal(n = 8, name = "Dark2")
        f <- list(
          family = "Courier New, monospace",
          size = 18,
          color = jBrewColors[1]
        )

        p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
          add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines",
                    line = list(color = jBrewColors[2], size = 3))%>%
          add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                    mode = 'markers',
                    marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                    mode = 'markers',
                    marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
          add_trace(x = ~grid,y = ~sapply(grid,spikes.loc.param, arg.others), name = 'mean function',
                    mode = 'lines',
                    line = list(color = jBrewColors[5], size = 3))%>%
          layout(xaxis = list(title = "Grid",titlefont = f),
                 yaxis = list(title = "Spiky Data",titlefont = f))

      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height, plot = p)
      return(output)
    }
    if(spikes.loc.dist == "normal"){
      if(any(is.na(spikes.loc.param))) stop('Need \'spikes.loc.param\'.')
      if(all(!is.na(spikes.loc.param))){
        if (length(spikes.loc.param) != 2) stop('\'spikes.loc.param\' must be of length 2.')
        if (spikes.loc.param[2] < 0) stop('Invalid \'spikes.loc.param\', sd must be positive')
        if ((spikes.loc.param[1] - 3*spikes.loc.param[2]) < range[1]|(spikes.loc.param[1] + 2*spikes.loc.param[2]) > range[2]) warning('Large prior sd might result in excluding out-of-range spikes')
      }
      prior.mean = spikes.loc.param[1]
      prior.sd = spikes.loc.param[2]
      ###Generate the spikes
      grid = seq(range[1],range[2],by = by)
      spikes.loc = rnorm(nspikes, mean = prior.mean, sd = prior.sd)    #spikes location/index in grid rather
      spikes.loc = spikes.loc - (min(spikes.loc) - range[1])  ##transform spikes location to have them in the range
      spikes.loc = spikes.loc[spikes.loc < range[2]]  ##remove those beyond the range
      grid = sort(unique(c(grid,spikes.loc)), decreasing = FALSE)
      ngrid = length(grid)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial part
      if (missing(smooth)){
        degree = length(poly.coefs) - 1
        smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      }
      ###generate the data
      y = smooth+rnorm(ngrid,0,noise.sd)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      if(plot == "yes"){
        layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
        plot(grid,smooth, type = "l", ylim = c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "")
        title(sub = "Curves + Random Spikes w/ Same Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = 2.5)
        points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        plot(data$grid, data$y, type = "p", ylim=c(min(smooth) - height, max(smooth)+height), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        title(main = "Data: Curves + Random Spikes w/ Same Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height)
      return(output)
    }
    stop('\'spikes.loc.dist\' other than \'uniform\',\'homopp\',\'nonhomopp\',\'normal\' is not available.')
  }
  # From here on is code for random spikes height, for now we only consider homopp and nonhomopp distribution for spikes location
  # This part of the code is not polished and is not used in our smoothEM paper. It might contain errors
  if(spikes.dist != "fixed"){
    if(spikes.loc.dist == "homopp"){
      height.gen=NA #If height.gen is unchanged in the next few 'if' cases, it means user enters an invalid spikes.dist
      if(spikes.dist == "normal"){
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) != 2) stop('\'spikes.param\' must be of length 2.')
          if (spikes.param[2] < 0) stop('Invalid \'spikes.param\', sd must be positive.')
        }
        height.mean = spikes.param[1]
        height.sd = spikes.param[2]
        height.gen = function(nspikes){
          return(rnorm(nspikes, height.mean, height.sd))
        }
      }
      if(spikes.dist == "chisq"){
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) > 1) warning('\'spikes.param\' has multiple elements. Only the first element will be used.')
          if (spikes.param[1] < 0) stop('Invalid \'spikes.param\', df must be positive.')
        }
        height.df = spikes.param[1]
        height.gen = function(nspikes){
          return(rchisq(nspikes,height.df))
        }
      }
      if(spikes.dist == "ncchisq"){ #i.e., non-central chi squared
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) != 2) stop('\'spikes.param\' need to be of length 2.')
          if (any(spikes.param < 0)) stop('Invalid \'spikes.param\', df and ncp must be positive.')
        }
        height.df = spikes.param[1]
        height.ncp = spikes.param[2]
        height.gen = function(nspikes){
          return(rchisq(nspikes,height.df,height.ncp))
        }
      }
      if(is.function(spikes.dist)){ #user specified heights
        warning('In \'spikes.dist\': First argument must be number of spikes to generate, others arguments contained in \'spikes.param\'.')
        height.gen = spikes.dist(nspikes,spikes.param)
      }
      if(!is.function(height.gen)) stop('Invalid \'spikes.dist\'.')
      if(any(is.na(spikes.loc.param))){
        if (is.na(nspikes)) stop('Must specify either \'spikes.loc.param\' or \'nspikes\'.')
        warning('Approximately ', nspikes, ' spikes will be generated.')
        want.nspikes = nspikes  #approximately how many spikes we want to generate
        pp.rate = want.nspikes/diff(range)  ##higher rate leads to higher nspikes
      }
      if(all(!is.na(spikes.loc.param))){
        if (spikes.loc.param[1] <= 0) stop('Invalid \'spikes.loc.param\'.')
        if (length(spikes.loc.param) > 1) warning('\'spikes.loc.param\' has multiple elements. Only the first element will be used')
        pp.rate = spikes.loc.param[1]
      }
      grid = seq(range[1],range[2],by = by)
      # Generate the spikes
      ## let numerator be approximate the number of spikes we want
      ## denominator is the range of x axis
      want.nspikes = pp.rate*diff(range) #in case spikes.loc.param is specified
      wait = rexp(want.nspikes + 100, rate = pp.rate)  #wait times of homogeneous poisson process are independent exponential
      ## we are generating more than we need
      spikes.loc= cumsum(wait)
      spikes.loc = spikes.loc - (min(spikes.loc) - range[1])  ##transform spikes location to have them in the range
      spikes.loc = spikes.loc[spikes.loc < range[2]]  ##remove those beyond the range
      for (i in (1:length(spikes.loc))){
        spikes.loc[i] = grid[which.min(abs(grid-spikes.loc[i]))]
      }
      spikes.loc=unique(spikes.loc)
      spikes.index = which(grid %in% spikes.loc)
      # Generate the polynomial
      if (missing(smooth)){
        degree = length(poly.coefs) - 1
        smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
      }
      # Generate data
      ngrid = length(grid)
      nspikes = length(spikes.index)
      y = smooth+rnorm(ngrid,0,noise.sd)
      height = height.gen(nspikes)
      y[spikes.index] = y[spikes.index] + height
      data = cbind.data.frame(y, grid)
      if(plot == "yes"){
        layout(matrix(c(1,2), nrow = 2), heights = c(10,10))
        par(bg = "lightyellow", mar=c(5,1,1,1), oma = c(0,0,0,0), pin = c(5.7,2.1))
        plot(grid,smooth, type = "l", ylim = c(min(smooth) - max(abs(height)), max(smooth)+max(abs(height))), main = "", xlab = "", ylab = "")
        title(sub = "Curves + HMPP Spikes w/ Normal Height + Noise", cex.sub = 1, font.sub =3, col.sub = "darkgreen", line = -2.5)
        points(grid[spikes.index], smooth[spikes.index] + height, col = "red", pch = 16)
        plot(data$grid, data$y, type = "p", ylim = c(min(smooth) - max(abs(height)), max(smooth)+max(abs(height))), main = "", xlab = "", ylab = "", cex = 0.2, col = "blue")
        title(main = "Data: Curves + HMPP Spikes w/ Normal Height + Noise", cex.main = 1, font.main =3, col.main = "darkgreen", line = -2.5)
      }
      output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data)
      return(output)
    }
    if(spikes.loc.dist == "nonhomopp"){
      ## Using thinning method by Lewis and Shedler (1978)
      ## Doesn't require integration of the mean function, so it's good even for complicated mean functions that are hard to integrate
      ## However we do need to be able to evaluate the mean function at the grid
      ## Generate non-homogeneous Poisson process (NHPP)
      if(spikes.dist != "normal"){
        stop('for nonhomopp \'spikes.loc.dist\', \'spikes.dist\' can only be \'normal\' or \'fixed\'')
      }
      if(spikes.dist == "normal"){
        if(any(is.na(spikes.param))) stop('Need \'spikes.param\'.')
        if(all(!is.na(spikes.param))){
          if (length(spikes.param) != 2) stop('\'spikes.param\' must be of length 2.')
          if (spikes.param[2] < 0) stop('Invalid \'spikes.param\', sd must be positive.')
        }
        height.mean = spikes.param[1]
        height.sd = spikes.param[2]
        # height.gen = function(nspikes){
        #   return(rnorm(nspikes, height.mean, height.sd))
        ## First, generate a homogeneous Poisson process with rate lambda_h = max(lambda), where h stands for homogeneous, the non-homogeneous is suppressed from lambda_nh to be just lambda
        ## generate the first grid, lambda will be calculated using this grid
        grid = seq(range[1],range[2],by = by)
        if (!is.function(spikes.loc.param)) stop('spikes.loc.param must be a function.')
        if (any(is.na(arg.others))) lambda = sapply(grid,spikes.loc.param) else lambda=sapply(grid,spikes.loc.param,arg.others)
        #if (any(is.na(arg.others))) lambda = spikes.loc.param(grid) else lambda = spikes.loc.param(grid, arg.others)
        lambda.h = max(lambda)
        wait.h = rexp(100000, rate = lambda.h)  ## wait times of homogeneous poisson process are independent exponential
        ## we are generating 1000000 wait times, we can adjust this number later
        spikes.h.loc = cumsum(wait.h) ## spikes.h.loc are spikes from homogeneous Poisson, candidates for deletion (or NHPP, depending on how you view it)
        for (i in (1:length(spikes.h.loc))){
          spikes.h.loc[i] = grid[which.min(abs(grid-spikes.h.loc[i]))]
        }
        spikes.h.loc=unique(spikes.h.loc)
        spikes.h.index = which(grid %in% spikes.h.loc)
        ## thinning requires knowing the ratio of lambda/lambda.h
        ## If if turns out that lambda[spikes.h] > lambda.h, that's okay, it means that we will not delete spikes at these locations.
        nspikes.h = length(spikes.h.loc)
        spikes.loc = c()
        i = 1
        while(i<=nspikes.h){
          if (runif(1,0,1)< (lambda[spikes.h.index[i]]/lambda.h)) spikes.loc = c(spikes.loc,spikes.h.loc[i])
          i = i+1
        }
        spikes.loc = spikes.loc[spikes.loc >= range[1]]  #we don't transform the spikes.loc here, because if we do so it will mismatch with the mean function
        spikes.loc = spikes.loc[spikes.loc <= range[2]]
        ## note that we are not doing (spikes.loc - range[1])/range[2], which will shrink the locations and make them not match with lambda later
        ## generate the final grid with NH-Poisson spikes locations
        ## I want to keep the naming of the variables used to generate final data the same as in other sections, so that the analysis part of the code can be recycled

        spikes.index = which(grid %in% spikes.loc)
        # Generate the polynomial
        if (missing(smooth)){
          degree = length(poly.coefs) - 1
          smooth = sapply(0:degree,function(i){grid^i})%*%matrix(poly.coefs, ncol = 1)
        }
        ngrid = length(grid)
        y = smooth+rnorm(ngrid,0,noise.sd)
        height = rnorm(length(spikes.loc), height.mean, height.sd)
        y[spikes.index] = y[spikes.index] + height
        data = cbind.data.frame(y, grid)
        p = NA
        if(plot == "yes" & any(is.na(arg.others))){
          jBrewColors <- brewer.pal(n = 8, name = "Dark2")
          f <- list(
            family = "Courier New, monospace",
            size = 18,
            color = jBrewColors[1]
          )

          p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
            add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines",
                      line = list(color = jBrewColors[2], size = 3))%>%
            add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                      mode = 'markers',
                      marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                      mode = 'markers',
                      marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid,y = ~spikes.loc.param(grid), name = 'mean function',
                      mode = 'lines',
                      line = list(color = jBrewColors[5], size = 3))%>%
            layout(xaxis = list(title = "Grid",titlefont = f),
                   yaxis = list(title = "Spiky Data",titlefont = f))
        }
        if(plot == "yes" & all(!is.na(arg.others))){
          jBrewColors <- brewer.pal(n = 8, name = "Dark2")
          f <- list(
            family = "Courier New, monospace",
            size = 18,
            color = jBrewColors[1]
          )

          p = plot_ly(data = data, type = "scatter", mode = "markers")%>%
            add_trace(x=~grid, y=~as.vector(smooth), name = 'smooth',mode = "lines",
                      line = list(color = jBrewColors[2], size = 3))%>%
            add_trace(x = ~grid[spikes.index],y = ~y[spikes.index], name = 'spikes',
                      mode = 'markers',
                      marker = list(color = jBrewColors[3], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid[-spikes.index],y = ~y[-spikes.index], name = 'smooth+noise',
                      mode = 'markers',
                      marker = list(color = jBrewColors[4], symbol = 'x', size = 3))%>%
            add_trace(x = ~grid,y = ~sapply(grid,spikes.loc.param, arg.others), name = 'mean function',
                      mode = 'lines',
                      line = list(color = jBrewColors[5], size = 3))%>%
            layout(xaxis = list(title = "Grid",titlefont = f),
                   yaxis = list(title = "Spiky Data",titlefont = f))

        }
        output = list(spikes.index = spikes.index, spikes.loc = spikes.loc, smooth = smooth, data = data, height = height,plot = p)
        return(output)
      }
    }
  }
}


# The following functions are the main ones that we use to separate spikes from smooth curve. Right now, we assume the distribution of white noise
# and also, the distribution of spike heights are normal. The main procedure is
# (1) to fit an initial spline estimates using smoothing splines
# (2) then under some arbitrary threshold, perform a rough initial separation.
# (3) feed this initial separation result to EM algorithm
# (4) after EM, we need to select a threshold p* to classify the spikes. In this step, use prior knowledge of the spike locations.

# The  init() function does (1) and (2)
init = function(data=NA, range, norder= 5, nbasis=NA, Lcoef = c(0,0,1), lambda = NA,alpha.prior = 0.4, smooth.true = NA,spikes.flag = NA, plot = TRUE,sigma.jiggle = NA, border.padding = FALSE){
  # data: a data frame that consists of the observed y and the grid over which it is observed. The first column must be y, and the second is grid.
  ## the data output by the function generate.data is already conforming.
  # range of spline basis functions
  # norder: the order of spline basis to use
  # nbasis: the number of spline bases to use
  # Lcoef: the coefficients for penalization terms in smoothing splines, defaulted to c(1,1). First element corresponds to 1st deriv, 2nd to 2nd.
  ## Penalization of higher order can be done by adding elements to the vector Lcoef.
  # lambda: a grid of tuning parameters to be used, must be of length 10. The length of this grid significantly affects the computing time (unles I optimize the code a little more)
  # smooth.true: if given, this is the true underlying curve, used to compared with the fitted splines. Only possible in simulations.
  # spikes.flag: if provided, these points will be excluded from the smoothing procedure. Can be useful in iterated smoothing and EM.
  # sigma.jiggle: the amount of jitter injected to flagged smooth component for each lambda to check for overfit (see later in code)
  # # border.padding: pad the two borders to avoid problems with fewer data there
  # nbins and qt (quantile) are used to correct the contaminated estimates. We will partition the x-axis into nbins bins, and
  ## within each bin, retain the data below this quantile and fit a smoothing spline. The result will be matched to the orifinal fit
  ## (using all data) in the non-spikes region.
  # VALUE: if smooth.true is given, the function returns a data frame that consists of the smooth estimates at each grid point, smooth true, and MSE
  ## otherwise, it returns only the smooth estimates.

  if(!is.data.frame(data)){
    stop('data must be of class data.frame')
  }
  if(dim(data)[2]>2){
    warning('data has more than 2 columns. Only the first two are used.')
  }
  if (dim(data)[2]<2){
    stop('data must have at least 2 columns. The first contains measurements and the second contains the grid over which measurements are taken.')
  }
  colnames(data) = c("y", "grid")
  ngrid = length(data$grid)
  if(is.na(norder)){
    stop('norder must be specified')
  }
  if(norder <= 0 | !is.numeric(norder)){
    stop('Invalid norder. Must be a positive number')
  }
  if(is.na(nbasis)){
    warning('nbasis is defaulted to ', ngrid + norder - 2, ' .')
    nbasis = (ngrid -1) + (norder - 1)
  }
  if(any(!is.na(smooth.true)) & length(smooth.true) != length(data$grid)){stop('\'smooth.true\' must be of the same length as grid.')}
  if(any(is.na(spikes.flag)) || length(spikes.flag) == 0){
    warning('either spikes.flag is not provided or it contains NA value. The entire dataset will be used to fit the splines. \n')
    smoothingdata = data  #this is the data we use to fit the curve.
    #if spikes.flag is provided, we exclude the spikes.flag. This is useful when we perform iterated EM.
  }else {smoothingdata = data[-spikes.flag,]}
  if (any(is.na(lambda))){lambda = 1/10^(0:9)}
  nlambda = length(lambda)
  spline.res = vector("list",nlambda)
  spikes.flag = vector("list",nlambda)
  smooth.data = vector("list",nlambda)
  change.jiggle = vector("list",nlambda) #this is explained below, used to prevent overfit
  spline.basis = create.bspline.basis(rangeval=range, nbasis=nbasis, norder = norder)  #create basis
  Lfd = vec2Lfd(Lcoef, range)
  if(is.na(sigma.jiggle)){
    loess_res = loess(y~grid, data = smoothingdata)$residuals
    sig.jiggle = median(abs(loess_res - median(loess_res)))
  } else {
    sig.jiggle = sigma.jiggle
  }
  if (border.padding){ #border padding might be useful if there're spikes around the boundaries
    grid = smoothingdata$grid
    ngrid = length(grid)
    y = smoothingdata$y
    pad.length = round(min(ngrid*0.3,20))
    left.border = grid[1]; right.border = grid[ngrid]
    grid.center.left = grid - left.border; grid.center.right = grid - right.border
    aug.grid.center.left = -grid.center.left[(pad.length+1):2]; aug.grid.center.right = -grid.center.right[(ngrid-1):(ngrid-pad.length)]
    aug.grid = c(aug.grid.center.left + left.border, grid, aug.grid.center.right+right.border)
    aug.y = c(y[(pad.length+1):2],y,y[(ngrid-1):(ngrid-pad.length)])
    aug.ngrid = length(aug.grid); aug.range = c(aug.grid[1], aug.grid[aug.ngrid])
    aug.spline.basis = create.bspline.basis(rangeval=aug.range, nbasis=nbasis, norder = norder)  #create basis
    aug.Lfd = vec2Lfd(Lcoef, aug.range)
    for (i in 1:nlambda){
      fdParobj = fdPar(spline.basis, Lfd, lambda[i]);aug.fdParobj = fdPar(aug.spline.basis, aug.Lfd, lambda[i])
      smooth.data.temp = smooth.basis(argvals = aug.grid,y = aug.y, fdParobj = aug.fdParobj)  #smooth the data
      spline.res.temp = (aug.y-eval.fd(aug.grid,smooth.data.temp$fd))[(pad.length+1):(aug.ngrid-pad.length)]
      sort.index = order(spline.res.temp, decreasing = FALSE)
      kmeans_result = kmeans(spline.res.temp,2)
      kmeans_spikes_id = which.max(kmeans_result$centers)
      if(mean(kmeans_result$cluster == kmeans_spikes_id) > alpha.prior){spikes.flag[[i]] = NA} else {
        spikes.flag[[i]] = which(kmeans_result$cluster == kmeans_spikes_id)
        smooth.data[[i]] = smooth.basis(argvals = smoothingdata$grid[-spikes.flag[[i]]],y = smoothingdata$y[-spikes.flag[[i]]], fdParobj = fdParobj)
        spline.res[[i]] = smoothingdata$y-eval.fd(smoothingdata$grid,smooth.data[[i]]$fd)
        #we will now also jiggle the smooth part and measure the amount of change in the fit, and use this as an overfit criterion
        nsmooth = length(data$y[-spikes.flag[[i]]])
        change.jiggle.it = c()
        # for (j in 1:nsmooth){ #this works too for smaller dataset
        #   smooth.jiggle.loo = smooth.basis(argvals = smoothingdata$grid[-spikes.flag[[i]]][-j],y = smoothingdata$y[-spikes.flag[[i]]][-j], fdParobj = fdParobj)
        #   smooth.jiggle.full = as.vector(eval.fd(smoothingdata$grid[-spikes.flag[[i]]],smooth.jiggle.loo$fd))
        #   smooth.data.full =  as.vector(eval.fd(smoothingdata$grid[-spikes.flag[[i]]],smooth.data[[i]]$fd))
        #   change.jiggle.it[j] = abs(smooth.jiggle.full[j] - smooth.data.full[j]) #mean instead of sum in case of unequal #smoothobservations
        # }
        for (j in 1:10){ #this works too for larger dataset
          smooth.jiggle = smooth.basis(argvals = data$grid[-spikes.flag[[i]]],y = (data$y[-spikes.flag[[i]]] + rnorm(nsmooth, 0, sig.jiggle)), fdParobj = fdParobj)
          smooth.jiggle.eval = as.vector(eval.fd(smoothingdata$grid[-spikes.flag[[i]]],smooth.jiggle$fd))
          smooth.data.eval =  as.vector(eval.fd(smoothingdata$grid[-spikes.flag[[i]]],smooth.data[[i]]$fd))
          change.jiggle.it[j] = mean(abs(smooth.jiggle.eval - smooth.data.eval)) #mean instead of sum in case of unequal #smoothobservations
        }
        change.jiggle[[i]] = mean(change.jiggle.it)
      }
    }
    lambda = lambda[!is.na(spikes.flag)]
    spline.res = spline.res[!is.na(spikes.flag)]
    smooth.data = smooth.data[!is.na(spikes.flag)]
    # plot(data$grid,eval.fd(data$grid,smooth.data[[9]]$fd), type = "l")
    # points(data$grid,data$y, col = "red")
    change.jiggle = change.jiggle[!is.na(spikes.flag)]
    spikes.flag = spikes.flag[!is.na(spikes.flag)] #this line needs to be last of all three because spikes.flag plays a role in previous 2 lines
  }else{
    for (i in 1:nlambda){
      fdParobj = fdPar(spline.basis, Lfd, lambda[i])
      smooth.data.temp = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)  #smooth the data
      spline.res.temp = smoothingdata$y-eval.fd(smoothingdata$grid,smooth.data.temp$fd)
      sort.index = order(spline.res.temp, decreasing = FALSE)
      kmeans_result = kmeans(spline.res.temp,2)
      kmeans_spikes_id = which.max(kmeans_result$centers)
      if(mean(kmeans_result$cluster == kmeans_spikes_id) > alpha.prior){spikes.flag[[i]] = NA} else {
        spikes.flag[[i]] = which(kmeans_result$cluster == kmeans_spikes_id)
        smooth.data[[i]] = smooth.basis(argvals = smoothingdata$grid[-spikes.flag[[i]]],y = smoothingdata$y[-spikes.flag[[i]]], fdParobj = fdParobj)
        spline.res[[i]] = smoothingdata$y-eval.fd(smoothingdata$grid,smooth.data[[i]]$fd)
        #we will now also jiggle the smooth part and measure the amount of change in the fit, and use this as an overfit criterion
        nsmooth = length(data$y[-spikes.flag[[i]]])
        change.jiggle.it = c()
        for (j in 1:10){ #this works too for larger dataset
          smooth.jiggle = smooth.basis(argvals = data$grid[-spikes.flag[[i]]],y = (data$y[-spikes.flag[[i]]] + rnorm(nsmooth, 0, sig.jiggle)), fdParobj = fdParobj)
          smooth.jiggle.eval = as.vector(eval.fd(smoothingdata$grid[-spikes.flag[[i]]],smooth.jiggle$fd))
          smooth.data.eval =  as.vector(eval.fd(smoothingdata$grid[-spikes.flag[[i]]],smooth.data[[i]]$fd))
          change.jiggle.it[j] = mean(abs(smooth.jiggle.eval - smooth.data.eval)) #mean instead of sum in case of unequal #smoothobservations
          # tempy = data$y[-spikes.flag[[i]]] + rnorm(nsmooth, 0, sig.jiggle)
          # tempx = data$grid[-spikes.flag[[i]]]
          # temp = ss(tempx, tempy, m = 2, lambda = lambda[i])
        }
        change.jiggle[[i]] = mean(change.jiggle.it)
      }
    }
    lambda = lambda[!is.na(spikes.flag)]
    spline.res = spline.res[!is.na(spikes.flag)]
    smooth.data = smooth.data[!is.na(spikes.flag)]
    change.jiggle = change.jiggle[!is.na(spikes.flag)]
    spikes.flag = spikes.flag[!is.na(spikes.flag)] #this line needs to be last of all three because spikes.flag plays a role in previous 2 lines
  }
  return(list(smooth.data = smooth.data,spline.res = spline.res, spikes.flag = spikes.flag,change.jiggle = change.jiggle, lambda= lambda))
}


## The mle() function uses EM to find MLE. For now, this applies when spikes sizes are normal and white noise is normal.
mle = function(data, spline.res, spikes.flag = NA, stop.crit = 1e-05, constraint = 0.05, alpha.prior = 0.4, VIOM = FALSE, MSOM = TRUE){
  #constraint: a lower bound on the variance of components to prevent degeneracy
  #alpha.prior: the prior on percentage of spikes. If EM classify more than alpha points as spikes, we reject this classification
  ## This parameter ensures that the code doesn't underestimate the smooth and (in the extreme case) classify every point as spikes
  #VIOM: variance inflated spikes
  #MSOM: mean shift spikes
  if(VIOM == TRUE & MSOM == TRUE){
    if(length(spikes.flag) == 0 || any(is.na(spikes.flag))){
      ngrid = length(data$grid)
      alpha.0 = 0
      sd.0 = sqrt(sum((spline.res)^2)/(length(spline.res)-1))  #we are not using the spikes.flag to estimate noise.sd here
      mu.h.0 = 0
      sigma.h.0 = constraint
    }else {
      ngrid = length(data$grid)
      alpha.0 = length(spikes.flag)/ngrid
      sd.0 = sqrt(sum((spline.res[-spikes.flag])^2)/(length(spline.res[-spikes.flag])-1))  #we are not using the spikes.flag to estimate noise.sd here
      mu.h.0 = mean(spline.res[spikes.flag])
      sigma.h.0 = sqrt(max(sum((spline.res[spikes.flag] - mu.h.0)^2)/length(spline.res[spikes.flag])-(sd.0^2), constraint))
    }

    EM = list(alpha = c(),sd = c(),mu.h = c(),sigma.h = c(), prob = c()) #inefficient storage scheme. Need review
    EM$alpha[1] = alpha.0
    EM$sd[1] = sd.0
    EM$mu.h[1] = mu.h.0
    EM$sigma.h[1] = sigma.h.0

    it = 1 #keep track of iteration
    test = 1 #measures the convergence of estimates, to be compared with stopping criteria
    crit = stop.crit  #stopping criteria for EM
    while(test > crit){
      cat("EM it = ", it, "\n")
      alpha = EM$alpha[it]
      sd = EM$sd[it]
      mu.h = EM$mu.h[it]
      sigma.h = EM$sigma.h[it]
      #E step:
      EM.prob = sapply(1:ngrid, function(i){
        component.1 = dnorm(spline.res[i],0, sd)*(1-alpha)  #joint density of data res.i and z.i1 (resi belongs to component 1)
        component.2 = dnorm(spline.res[i],mu.h, sqrt(sigma.h^2 + sd^2))*alpha
        prob.1 = component.1/(component.1+component.2)  #This is probability of residual belonging to N(0,sd) given observed data and current estimate of param
        return(c(prob.1, 1-prob.1))
      })
      EM$prob = EM.prob[2,]  #save the probability of belonging to spike component in the last iteration
      #M step:
      EM$alpha[it+1] = sum(EM.prob[2,])/sum(EM.prob)
      EM$mu.h[it+1] = ifelse(sum(EM.prob[2,])>0,sum(spline.res*EM.prob[2,])/sum(EM.prob[2,]),0)
      propose.sigma2.h = ifelse(sum(EM.prob[2,]) >0,sum(EM.prob[2,]*(spline.res-EM$mu.h[it])^2)/sum(EM.prob[2,]) - (EM$sd[it]^2), constraint)
      EM$sigma.h[it+1] = ifelse(propose.sigma2.h<constraint, sqrt(constraint), sqrt(propose.sigma2.h))

      # #solve cubic polynomial in noise.sd^2
      c3 = -sum(EM.prob) #coef of noise.sd^6
      c2 = sum(EM.prob[1,]*(spline.res^2)) - 2*(EM$sigma.h[it+1]^2)*sum(EM.prob[1,]) + sum(EM.prob[2,]*(spline.res - EM$mu.h[it+1])^2) - (EM$sigma.h[it+1]^2)*sum(EM.prob[2,])
      c1 = 2*(EM$sigma.h[it+1]^2)*sum(EM.prob[1,]*(spline.res^2)) - (EM$sigma.h[it+1]^4)*sum(EM.prob[1,])
      c0 = (EM$sigma.h[it+1]^4)*sum(EM.prob[1,]*(spline.res^2))
      # manual code to solve for the polynomials
      # a2 = c2/c3
      # a1 = c1/c3
      # a0 = c0/c3
      #
      # Q = (3*a1-a2^2)/9
      # R = (9*a2*a1 - 27*a0-2*a2^3)/54
      # D = Q^3 + R^2
      # if(D > 0){
      #   S = (R+sqrt(D))^(1/3)
      #   T = ifelse(is.nan((R-sqrt(D))^(1/3)),0,(R-sqrt(D))^(1/3))
      #   roots = -a2/3 + (S + T)
      # }
      # if(D<0){
      #   argcos = acos(R/sqrt(-Q^3))
      #   roots = c(2*sqrt(-Q)*cos(argcos/3)-a2/3,2*sqrt(-Q)*cos((argcos+2*pi)/3)-a2/3,2*sqrt(-Q)*cos((argcos+4*pi)/3)-a2/3)
      # }
      # if(D == 0){
      #   S = R^(1/3)
      #   roots = c(-a2/3 + 2*S,-a2/3 - S)
      # }
      # We can use polyroot instead
      comp.roots = polyroot(c(c0,c1,c2,c3))
      real.roots = Re(comp.roots)[abs(Im(comp.roots)) < 1e-6]
      EM$sd[it+1] = sqrt(real.roots[real.roots>0])

      test = sum(sapply(1:4, function(k) {
        abs(EM[[k]][[it+1]] - EM[[k]][[it]])
      }))
      it = it+1
    }
    # calculate predictive loglikelihood: expectation of the loglikelihood, taken over membership vector z;
    # doesn't depend on classification results, we think of this as a function of membership probabilities
    llh.pred = sum(sapply(1:ngrid, function(i){
      ifelse((1-EM$prob[i])>0,(1-EM$prob[i])*(log((1-EM$alpha[it])*dnorm(spline.res[i], 0, EM$sd[it]))),0) + ifelse(EM$prob[i] > 0,EM$prob[i]*(log(EM$alpha[it]*dnorm(spline.res[i], EM$mu.h[it], sqrt(EM$sd[it]^2 + EM$sigma.h[it]^2)))),0)
    })
    )
    threshold = seq(0.5,1,length.out = 6)
    spikes.EM.cand = vector("list", 5)
    loglike.cand = rep(NA, 5)
    check = rep(TRUE,5) #a candidate is admitted if it contains less than 80% of data points
    for (i in (1:5)){
      spikes.EM.cand[[i]] = which(EM$prob>threshold[i])
      loglike.cand[i] = sum(log(dnorm(spline.res[spikes.EM.cand[[i]]],EM$mu.h[it], sqrt((EM$sigma.h[it])^2 + (EM$sd[it])^2))))+sum(log(dnorm(spline.res[-spikes.EM.cand[[i]]],0, EM$sd[it])))
      if (length(spikes.EM.cand[[i]]) > round(alpha.prior*ngrid)) {check = FALSE}
    }
    if(any(check)){
      best = which.max(loglike.cand[check])
      spikes.EM = spikes.EM.cand[[best]]
      loglike = loglike.cand[best]
    }
    else {
      spikes.EM = c()
      loglike = sum(log(dnorm(spline.res,0, EM$sd[it])))
    }

    return(list(loglike = loglike,llh.pred=llh.pred,param = c(alpha.EM = EM$alpha[it], height.EM = EM$mu.h[it], sigma.h.EM = EM$sigma.h[it],sd.EM = EM$sd[it]),spikes.EM = spikes.EM,prob.EM = EM$prob))
  }
  if (VIOM == FALSE & MSOM == TRUE){
    if(length(spikes.flag) == 0 || any(is.na(spikes.flag))){
      ngrid = length(data$grid)
      alpha.0 = 0
      sd.0 = sqrt(sum((spline.res)^2)/(length(spline.res)-1))  #we are not using the spikes.flag to estimate noise.sd here
      mu.h.0 = 0
    }else {
      ngrid = length(data$grid)
      alpha.0 = length(spikes.flag)/ngrid
      mu.h.0 = mean(spline.res[spikes.flag])
      sd.0 = sqrt((sum((spline.res[-spikes.flag])^2) + sum((spline.res[spikes.flag]-mu.h.0)^2))/ngrid)
    }

    EM = list(alpha = c(),sd = c(),mu.h = c(),prob = c()) #inefficient storage scheme. Need review
    EM$alpha[1] = alpha.0
    EM$sd[1] = sd.0
    EM$mu.h[1] = mu.h.0

    it = 1 #keep track of iteration
    test = 1 #measures the convergence of estimates, to be compared with stopping criteria
    crit = stop.crit  #stopping criteria for EM
    while(test > crit){
      alpha = EM$alpha[it]
      sd = EM$sd[it]
      mu.h = EM$mu.h[it]
      #E step:
      EM.prob = sapply(1:ngrid, function(i){
        component.1 = dnorm(spline.res[i],0, sd)*(1-alpha)  #joint density of data res.i and z.i1 (resi belongs to component 1)
        component.2 = dnorm(spline.res[i],mu.h, sd)*alpha
        prob.1 = component.1/(component.1+component.2)  #This is probability of residual belonging to N(0,sd) given observed data and current estimate of param
        return(c(prob.1, 1-prob.1))
      })
      EM$prob = EM.prob[2,]  #save the probability of belonging to spike component in the last iteration
      #M step:
      EM$alpha[it+1] = sum(EM.prob[2,])/sum(EM.prob)
      EM$mu.h[it+1] = ifelse(sum(EM.prob[2,])>0,sum(spline.res*EM.prob[2,])/sum(EM.prob[2,]),0)
      EM$sd[it+1] = sqrt((sum((spline.res^2)*EM.prob[1,]) + sum(((spline.res-EM$mu.h[it+1])^2)*EM.prob[2,]))/sum(EM.prob))

      test = sum(sapply(1:3, function(k) {
        abs(EM[[k]][[it+1]] - EM[[k]][[it]])
      }))
      #cat("it = ", it, " test = ", test, "\n")
      it = it+1
    }
    # calculate predictive loglikelihood: expectation of the loglikelihood, taken over membership vector z;
    # doesn't depend on classification results, we think of this as a function of membership probabilities
    llh.pred = sum(sapply(1:ngrid, function(i){
      ifelse((1-EM$prob[i])>0,(1-EM$prob[i])*(log((1-EM$alpha[it])*dnorm(spline.res[i], 0, EM$sd[it]))),0) + ifelse(EM$prob[i] > 0,EM$prob[i]*(log(EM$alpha[it]*dnorm(spline.res[i], EM$mu.h[it], EM$sd[it]))),0)
    })
    )
    threshold = seq(0.5,1,length.out = 6)
    spikes.EM.cand = vector("list", 5)
    loglike.cand = rep(NA, 5)
    check = rep(TRUE,5) #a candidate is admitted if it contains less than 30% of data points
    for (i in (1:5)){
      spikes.EM.cand[[i]] = which(EM$prob>threshold[i])
      loglike.cand[i] = sum(log(dnorm(spline.res[spikes.EM.cand[[i]]],EM$mu.h[it], EM$sd[it])))+sum(log(dnorm(spline.res[-spikes.EM.cand[[i]]],0, EM$sd[it])))
      if (length(spikes.EM.cand[[i]]) > round(alpha.prior*ngrid)) {check = FALSE}
    }
    if(any(check)){
      best = which.max(loglike.cand[check])
      spikes.EM = spikes.EM.cand[[best]]
      loglike = loglike.cand[best]
    }
    else {
      spikes.EM = c()
      loglike = sum(log(dnorm(spline.res,0, EM$sd[it])))
    }
    return(list(loglike = loglike,llh.pred=llh.pred,param = c(alpha.EM = EM$alpha[it], height.EM = EM$mu.h[it],sd.EM = EM$sd[it]),spikes.EM = spikes.EM,prob.EM = EM$prob))
  }
}

## This is the main function that puts together the procedure. It uses both the init() and mle() functions
classify = function(data, range, norder = 4, nbasis=NA,lambda = NA, Lcoef = c(0,0,1), spikes.flag = NA,smooth.true = NA,constraint = 0.5, alpha.prior= 0.3,sigma.jiggle = NA, beta =1, MSOM = TRUE, VIOM = FALSE,plot = FALSE,border.padding = FALSE){
  # data: is the usual data frame that contains the grid and y values
  # smooth.true: if provided (only possible in simulation setting), plot the estimate next to the true smooth function
  ###the function returns the best iterated mle estimates. That is, in each iteration, we select the best initialization with the
  ###highest predictive soft likelihood (treated as a function of membership vector z in EM). Then, classify spikes such that the
  ###classification maximizes the hard likelihood.
  # constraint, alpha.prior: these are passed onto mle() function
  # beta: the weight for F(lambda) in step 5 of the algorithm (see manuscript). Set to be 1, but can be adjusted to prevent overfit based on prior knowledge of f
  if(!is.data.frame(data)){
    stop('data must be of class data.frame')
  }
  if(dim(data)[2]>2){
    warning('data has more than 2 columns. Only the first two are used.')
  }
  if (dim(data)[2]<2){
    stop('data must have at least 2 columns. The first contains measurements and the second contains the grid over which measurements are taken.')
  }
  colnames(data) = c("y", "grid")
  ngrid = length(data$grid)
  if(is.na(norder)){
    stop('norder must be specified')
  }
  if(norder <= 0 | !is.numeric(norder)){
    stop('Invalid norder. Must be a positive number')
  }
  if(is.na(nbasis)){
    warning('nbasis is defaulted to ', ngrid + norder - 2, ' .')
    nbasis = (ngrid -1) + (norder - 1)
  }
  if(any(!is.na(smooth.true)) & length(smooth.true) != length(data$grid)){stop('\'smooth.true\' must be of the same length as grid.')}

  #Obtain initial smooth fit and spikes flag
  init1 = init(data = data,range = range, norder = norder, lambda = lambda,nbasis = nbasis, Lcoef = Lcoef, alpha.prior = alpha.prior, smooth.true = smooth.true, spikes.flag = spikes.flag,sigma.jiggle = sigma.jiggle,border.padding =border.padding, plot = plot)
  ninit = length(init1$spikes.flag)
  if (ninit == 0) {
    cat("cat1 \t")
    smooth.data = fit(data = data,range = range, norder = norder, nbasis = nbasis, Lcoef = Lcoef, lambda = lambda,plot = FALSE) #no smooth.true provide means calculating smoothdata w gcv lambda
    res = data$y - smooth.data
    best = mle(data = data, spline.res = res, stop.crit = 1e-03, constraint = constraint, alpha.prior= alpha.prior, MSOM= MSOM, VIOM = VIOM)
    flag.spikes = best$spikes.EM
    lamb = NA
    smooth.fit = NA
    change.jiggle = NA
  }else{
    cat("cat2 \t")
    em = vector("list",ninit)
    loglike = vector("list",ninit)
    llh.pred = vector("list",ninit)
    #Run MLE with different initialization
    for(i in 1:ninit){
      em[[i]] = mle(data = data, spline.res = init1$spline.res[[i]], spikes.flag = init1$spikes.flag[[i]], stop.crit = 1e-03, constraint = constraint, alpha.prior= alpha.prior, MSOM= MSOM, VIOM = VIOM)
      llh.pred[i] = em[[i]]$llh.pred
      loglike[i] = em[[i]]$loglike
      cat('loglike = ', em[[i]]$loglike,'\t')
    }
    best.ind = which.max(rank(unlist(loglike))+beta*rank(-unlist(init1$change.jiggle))) #scale change.jiggle so that the two terms are balanced
    cat('best.ind = ', best.ind, '\n')
    flag.spikes = em[[best.ind]]$spikes.EM
    best = em[[best.ind]]
    lamb = init1$lambda[best.ind]
    smooth.fit = init1$smooth.data[[best.ind]]
    change.jiggle = init1$change.jiggle[best.ind]
  }
  cat('length(flag) ', length(flag.spikes),'\n')
    return(list(smooth.fit = smooth.fit, EM.result = best, lambda = lamb,change.jiggle = change.jiggle))
}


# This function provide the final fit based on the classification from the procedure above. It also calculates some goodness of fit diagnostics.
fit = function(data, range, norder = 5, nbasis=NA, Lcoef = c(0,0,1), smooth.true = NA, spikes.flag = NA, lambda = NA, plot = FALSE, border.padding = FALSE){
  # data = heatwave; nbasis=NA;smooth.true = NA;range = c(1910,2015); norder = 4; Lcoef = c(0,1); spikes.flag = EM.result$EM.result$spikes.EM; lambda = NA; plot = FALSE;border.padding = FALSE
  # if smooth.ind is provided (only possible in simulations), then return the smooth residuals.
  # Lcoef starts with differential order 0, i.e.
  # Lx(t) = b0(t)x(t) + b1(t)Dx(t) + b2(t)(D^2)x(t) + ... + bm(t)(D^m)x(t)
  # then  Lcoef = c(0,0,1) means b0 = b1 = 0, b2 = 1
  if(!is.data.frame(data)){
    stop('data must be of class data.frame')
  }
  if(dim(data)[2]>2){
    warning('data has more than 2 columns. Only the first two are used.')
  }
  if (dim(data)[2]<2){
    stop('data must have at least 2 columns. The first contains measurements and the second contains the grid over which measurements are taken.')
  }
  colnames(data) = c("y", "grid")
  ngrid = length(data$grid)
  if(is.na(norder)){
    stop('norder must be specified')
  }
  if(norder <= 0 | !is.numeric(norder)){
    stop('Invalid norder. Must be a positive number')
  }
  if(is.na(nbasis)){
    warning('nbasis is defaulted to ', ngrid + norder - 2, ' .')
    nbasis = (ngrid -1) + (norder - 1)
  }
  if(any(!is.na(smooth.true)) & length(smooth.true) != length(data$grid)){stop('\'smooth.true\' must be of the same length as grid.')}
  spline.basis = create.bspline.basis(rangeval=range, nbasis=nbasis, norder = norder)  #create basis
  Lfd = vec2Lfd(Lcoef, range)
  if(any(is.na(spikes.flag)) || length(spikes.flag)== 0){
    warning('either spikes.flag is not provided or it contains NA value. The entire dataset will be used to fit the splines. \n')
    smoothingdata = data  #this is the data we use to fit the curve.
    #if spikes.flag is provided, we exclude the spikes.flag
  }else {smoothingdata = data[-spikes.flag,]}
  if (any(is.na(lambda))){
    cat(paste('fitting smoothing spline: calculating gcv lambda... \n'))
    lam = 0:9
    nlam = length(lam)
    dfsave = rep(NA,nlam)
    names(dfsave) = lam
    gcvsave = dfsave
    for (ilam in 1:nlam) {
      #cat(paste('lambda =',10^-lam[ilam],'\n'))
      lamb = 10^-lam[ilam]
      fdParobj = fdPar(spline.basis, Lfd, lamb)
      smoothlist = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj, dfscale = 1.2)
      dfsave[ilam] = smoothlist$df
      gcvsave[ilam] = sum(smoothlist$gcv)
    }
    #plot(lam, gcvsave, type='b', lwd=2)
    lambda.min.gcv = 10^(-lam[which.min(gcvsave)[1]])  #which.min(gcvsave)[1] because there might be more than 1 min
    cat(paste('fitting smoothing spline with gcv lambda = ',lambda.min.gcv,'.\n'))
  }else{
    cat(paste('fitting smoothing spline: calculating gcv lambda... \n'))
    nlam = length(lambda)
    dfsave = rep(NA,nlam)
    gcvsave = dfsave
    for (ilam in 1:nlam) {
      fdParobj = fdPar(spline.basis, Lfd, lambda[ilam])
      smoothlist = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj, dfscale = 1.2)
      dfsave[ilam] = smoothlist$df
      gcvsave[ilam] = sum(smoothlist$gcv)
    }
    lambda.min.gcv = lambda[which.min(gcvsave)[1]]  #which.min(gcvsave)[1] because there might be more than 1 min
    cat(paste('fitting smoothing spline with gcv lambda = ',lambda.min.gcv,'.\n'))
  }
  if (border.padding){
    grid = smoothingdata$grid
    ngrid = length(grid)
    y = smoothingdata$y
    pad.length = round(min(ngrid*0.3,20))
    left.border = grid[1]; right.border = grid[ngrid]
    grid.center.left = grid - left.border; grid.center.right = grid - right.border
    aug.grid.center.left = -grid.center.left[(pad.length+1):2]; aug.grid.center.right = -grid.center.right[(ngrid-1):(ngrid-pad.length)]
    aug.grid = c(aug.grid.center.left + left.border, grid, aug.grid.center.right+right.border)
    aug.y = c(y[(pad.length+1):2],y,y[(ngrid-1):(ngrid-pad.length)])
    aug.ngrid = length(aug.grid); aug.range = c(aug.grid[1], aug.grid[aug.ngrid])
    aug.spline.basis = create.bspline.basis(rangeval=aug.range, nbasis=nbasis, norder = norder)  #create basis
    aug.Lfd = vec2Lfd(Lcoef, aug.range)
    aug.fdParobj = fdPar(aug.spline.basis, aug.Lfd, lambda = lambda.min.gcv)
    aug.smooth.fd = smooth.basis(argvals = aug.grid,y = aug.y, fdParobj = aug.fdParobj)  #smooth the data
    smooth = as.vector(eval.fd(data$grid,aug.smooth.fd$fd))# [(pad.length+1):(aug.ngrid-pad.length)] #have to be evaluated at the original data's grid
    if(all(!is.na(smooth.true))){
      smooth.res = smooth.true-smooth
      return(list(l1 = mean(abs(smooth.res)), l2 = mean(smooth.res^2), linf = max(abs(smooth.res)), smooth.res = smooth.res))
    }else{
      return(smooth)
    }
  }else{
    fdParobj = fdPar(spline.basis, Lfd, lambda = lambda.min.gcv)
    smooth.fd = smooth.basis(argvals = smoothingdata$grid,y = smoothingdata$y, fdParobj = fdParobj)  #smooth the data
    smooth = as.vector(eval.fd(data$grid,smooth.fd$fd)) #have to be evaluated at the original data's grid
    if(all(!is.na(smooth.true))){
      smooth.res = smooth.true-smooth
      return(list(l1 = mean(abs(smooth.res)), l2 = mean(smooth.res^2), linf = max(abs(smooth.res)), smooth.res = smooth.res))
    }else{
      return(smooth)
    }
  }
}
